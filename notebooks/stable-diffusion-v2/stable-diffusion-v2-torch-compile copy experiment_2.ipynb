{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion v2 Demo with Torch Compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Diffusion v2 for Text-to-Image Generation\n",
    "\n",
    "To start, let's look on Text-to-Image process for Stable Diffusion v2. We will use [Stable Diffusion v2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1) model for these purposes. The main difference from Stable Diffusion v2 and Stable Diffusion v2.1 is usage of more data, more training, and less restrictive filtering of the dataset, that gives promising results for selecting wide range of input text prompts. More details about model can be found in [Stability AI blog post](https://stability.ai/blog/stablediffusion2-1-release7-dec-2022) and original model [repository](https://github.com/Stability-AI/stablediffusion).\n",
    "\n",
    "### Stable Diffusion in Diffusers library\n",
    "To work with Stable Diffusion v2, we will use Hugging Face [Diffusers](https://github.com/huggingface/diffusers) library. To experiment with Stable Diffusion models, Diffusers exposes the [`StableDiffusionPipeline`](https://huggingface.co/docs/diffusers/using-diffusers/conditional_image_generation) similar to the [other Diffusers pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview).  The code below demonstrates how to create `StableDiffusionPipeline` using `stable-diffusion-2-1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu \"torch>=2.1,<2.4\" \"torchvision<0.19.0\" \"diffusers>=0.18.0\" \"invisible-watermark>=0.2.0\" \"transformers>=4.33.0\" \"accelerate\" \"onnx\" \"peft==0.6.2\"\n",
    "%pip install -q \"openvino>=2023.1.0\" \"gradio>=4.19\"\n",
    "%pip install git+https://github.com/anzr299/nncf.git@fx_compress_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch._export import capture_pre_autograd_graph\n",
    "from nncf.torch.dynamic_graph.patch_pytorch import disable_patching\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "import random\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(42)\n",
    "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\")\n",
    "pipe.to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Models to Torch Fx Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_input = torch.ones((1, 77), dtype=torch.long)\n",
    "text_encoder_2_input = torch.ones((1, 77), dtype=torch.long)\n",
    "vae_encoder_input = torch.ones((1, 3, 256, 256))\n",
    "vae_decoder_input = torch.ones((1, 4, 128, 128))\n",
    "vae_decoder_kwargs = {}\n",
    "vae_decoder_kwargs[\"return_dict\"] = False\n",
    "\n",
    "latents_shape = (2, 4, 128, 128)\n",
    "latents = torch.randn(latents_shape)\n",
    "t = torch.from_numpy(np.array(1, dtype=np.float32))\n",
    "added_cond_kwargs = {}\n",
    "added_cond_kwargs[\"text_embeds\"] = torch.ones((2, 1280))\n",
    "added_cond_kwargs[\"time_ids\"] = torch.ones((2,6))\n",
    "unet_kwargs = {}\n",
    "unet_kwargs[\"encoder_hidden_states\"] = torch.ones((2, 77, 2048))\n",
    "unet_kwargs[\"added_cond_kwargs\"] = added_cond_kwargs\n",
    "unet_kwargs[\"return_dict\"] = False\n",
    "unet_input = (latents, t)\n",
    "\n",
    "text_encoder_kwargs = {}\n",
    "text_encoder_kwargs['output_hidden_states'] = True\n",
    "\n",
    "with torch.no_grad():\n",
    "    with disable_patching():\n",
    "        pipe.text_encoder = capture_pre_autograd_graph(pipe.text_encoder.eval(), args=(text_encoder_input,), kwargs=(text_encoder_kwargs))\n",
    "        pipe.text_encoder_2 = capture_pre_autograd_graph(pipe.text_encoder_2.eval(), args=(text_encoder_2_input,), kwargs=(text_encoder_kwargs))\n",
    "        pipe.vae.encoder = capture_pre_autograd_graph(pipe.vae.encoder.eval(), args=(vae_encoder_input,))\n",
    "        pipe.vae.decoder  = capture_pre_autograd_graph(pipe.vae.decoder.eval(), args=(vae_decoder_input,))\n",
    "        pipe.unet = capture_pre_autograd_graph(pipe.unet.eval(), args=(*unet_input,), kwargs=(unet_kwargs))\n",
    "del added_cond_kwargs\n",
    "del unet_kwargs\n",
    "del unet_input\n",
    "del latents\n",
    "del t\n",
    "del vae_encoder_input\n",
    "del vae_decoder_input\n",
    "del text_encoder_2_input\n",
    "del text_encoder_input\n",
    "del text_encoder_kwargs\n",
    "del vae_decoder_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect Calibration Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Any, Dict, List\n",
    "import torch\n",
    "\n",
    "def disable_progress_bar(pipeline, disable=True):\n",
    "    if not hasattr(pipeline, \"_progress_bar_config\"):\n",
    "        pipeline._progress_bar_config = {'disable': disable}\n",
    "    else:\n",
    "        pipeline._progress_bar_config['disable'] = disable\n",
    "\n",
    "\n",
    "class UNetWrapper(torch.nn.Module):\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.captured_args = []\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        if np.random.rand() <= 0.7:\n",
    "            self.captured_args.append((*args, *tuple(kwargs.values())))\n",
    "        return self.unet(*args, **kwargs)\n",
    "\n",
    "def collect_calibration_data(ov_pipe, calibration_dataset_size: int, num_inference_steps: int) -> List[Dict]:\n",
    "    \n",
    "    original_unet = ov_pipe.transformer\n",
    "    calibration_data = []\n",
    "    disable_progress_bar(ov_pipe)\n",
    "    \n",
    "    dataset = datasets.load_dataset(\"google-research-datasets/conceptual_captions\", split=\"train\", trust_remote_code=True).shuffle(seed=42)\n",
    "\n",
    "    pipe_copy = ov_pipe\n",
    "    wrapped_unet = UNetWrapper(ov_pipe.transformer)\n",
    "    pipe_copy.transformer = wrapped_unet\n",
    "    # Run inference for data collection\n",
    "    pbar = tqdm(total=calibration_dataset_size)\n",
    "    for i, batch in enumerate(dataset):\n",
    "        prompt = batch[\"caption\"]\n",
    "        print(prompt)\n",
    "        if len(prompt) > ov_pipe.tokenizer.model_max_length:\n",
    "            continue\n",
    "        # Run the pipeline\n",
    "        ov_pipe(prompt, num_inference_steps=num_inference_steps)\n",
    "        calibration_data.extend(wrapped_unet.captured_args)\n",
    "        wrapped_unet.captured_args = []\n",
    "        pbar.update(len(calibration_data) - pbar.n)\n",
    "        if pbar.n >= calibration_dataset_size:\n",
    "            break\n",
    "\n",
    "    disable_progress_bar(ov_pipe, disable=False)\n",
    "    pipe_copy.transformer = original_unet\n",
    "    ov_pipe = pipe_copy\n",
    "    return calibration_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_ops_with_weights(graph_module):\n",
    "    ops_with_weights = []\n",
    "    for node in graph_module.graph.nodes:\n",
    "        if \"linear\" in node.name:\n",
    "            ops_with_weights.append(node.name)\n",
    "    return ops_with_weights\n",
    "\n",
    "calibration_dataset_size = 30\n",
    "unet_calibration_data = collect_calibration_data(pipe,\n",
    "                                                    calibration_dataset_size=calibration_dataset_size,\n",
    "                                                    num_inference_steps=20)\n",
    "unet_ignored_scope = collect_ops_with_weights(pipe.unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nncf\n",
    "from nncf.quantization.advanced_parameters import AdvancedSmoothQuantParameters\n",
    "from nncf.quantization.range_estimator import RangeEstimatorParametersSet\n",
    "\n",
    "def disable_progress_bar(pipeline, disable=True):\n",
    "    if not hasattr(pipeline, \"_progress_bar_config\"):\n",
    "        pipeline._progress_bar_config = {'disable': disable}\n",
    "    else:\n",
    "        pipeline._progress_bar_config['disable'] = disable\n",
    "\n",
    "with disable_patching():\n",
    "    with torch.no_grad():\n",
    "        nncf.compress_weights(pipe.text_encoder)\n",
    "        nncf.compress_weights(pipe.text_encoder_2)\n",
    "        nncf.compress_weights(pipe.vae.encoder)\n",
    "        nncf.compress_weights(pipe.vae.decoder)\n",
    "        quantized_unet = nncf.quantize( #2\n",
    "            model=pipe.unet,\n",
    "            calibration_dataset=nncf.Dataset(unet_calibration_data),\n",
    "            subset_size=len(unet_calibration_data),\n",
    "            model_type=nncf.ModelType.TRANSFORMER,\n",
    "            ignored_scope=nncf.IgnoredScope(names=unet_ignored_scope),\n",
    "            advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alpha=-1, disable_bias_correction=True, weights_range_estimator_params=RangeEstimatorParametersSet.MINMAX, activations_range_estimator_params=RangeEstimatorParametersSet.MINMAX)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Models with OV Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.unet = torch.compile(quantized_unet, backend='openvino')\n",
    "pipe.text_encoder = torch.compile(pipe.text_encoder, backend='openvino')\n",
    "pipe.text_encoder_2 = torch.compile(pipe.text_encoder_2, backend='openvino')\n",
    "pipe.vae.encoder = torch.compile(pipe.vae.encoder, backend='openvino')\n",
    "pipe.vae.decoder = torch.compile(pipe.vae.decoder, backend='openvino')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(models):\n",
    "    total_size = 0\n",
    "    for model in models:\n",
    "        param_size = 0\n",
    "        for param in model.parameters():\n",
    "            param_size += param.nelement() * param.element_size()\n",
    "        buffer_size = 0\n",
    "        for buffer in model.buffers():\n",
    "            buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "        model_size_mb = (param_size + buffer_size) / 1024**2\n",
    "\n",
    "        total_size += model_size_mb\n",
    "    return total_size\n",
    "print(\"Unet Model Size: \", get_model_size([pipe.unet]))\n",
    "print(\"Pipeline Models Size: \", get_model_size([pipe.unet, pipe.vae.encoder, pipe.vae.decoder, pipe.text_encoder, pipe.text_encoder_2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warmup the model for initial compile\n",
    "prompt = \"cute cat 4k, high-res, masterpiece, best quality, soft lighting, dynamic angle\"\n",
    "# prompt = \"valley in the Alps at sunset, epic vista, beautiful landscape, 4k, 8k\"\n",
    "negative_prompt = \"frames, borderline, text, charachter, duplicate, error, out of frame, watermark, low quality, ugly, deformed, blur\"\n",
    "num_steps = 1\n",
    "with torch.no_grad():\n",
    "    image = pipe(prompt=prompt, negative_prompt=negative_prompt, num_inference_steps=num_steps, generator=generator).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "with torch.no_grad():\n",
    "    image = pipe(prompt=prompt, negative_prompt=negative_prompt, num_inference_steps=25, generator=generator).images[0]\n",
    "image.show()\n",
    "image.save(\"SDXL.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
