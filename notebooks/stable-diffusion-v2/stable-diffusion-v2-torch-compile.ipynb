{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion v2 Demo with Torch Compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"diffusers>=0.14.0\" openvino-nightly \"datasets>=2.14.6\" \"transformers>=4.25.1\" \"gradio>=4.19\" \"torch>=2.1\" Pillow opencv-python --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q git+https://github.com/openvinotoolkit/nncf.git\n",
    "%pip install -q accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Diffusion v2 for Text-to-Image Generation\n",
    "\n",
    "To start, let's look on Text-to-Image process for Stable Diffusion v2. We will use [Stable Diffusion v2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1) model for these purposes. The main difference from Stable Diffusion v2 and Stable Diffusion v2.1 is usage of more data, more training, and less restrictive filtering of the dataset, that gives promising results for selecting wide range of input text prompts. More details about model can be found in [Stability AI blog post](https://stability.ai/blog/stablediffusion2-1-release7-dec-2022) and original model [repository](https://github.com/Stability-AI/stablediffusion).\n",
    "\n",
    "### Stable Diffusion in Diffusers library\n",
    "To work with Stable Diffusion v2, we will use Hugging Face [Diffusers](https://github.com/huggingface/diffusers) library. To experiment with Stable Diffusion models, Diffusers exposes the [`StableDiffusionPipeline`](https://huggingface.co/docs/diffusers/using-diffusers/conditional_image_generation) similar to the [other Diffusers pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview).  The code below demonstrates how to create `StableDiffusionPipeline` using `stable-diffusion-2-1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05bdfee9ce504db69b64d4ba5dd4451e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Downloads/openvino_notebooks/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch \n",
    "import numpy as np\n",
    "from transformers import CLIPTokenizer\n",
    "from diffusers.schedulers import DDIMScheduler\n",
    "\n",
    "seed = 42\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\").to(\"cpu\")\n",
    "\n",
    "pipe.text_encoder = torch.compile(pipe.text_encoder.eval(), backend='openvino')\n",
    "pipe.unet = torch.compile(pipe.unet.eval(), backend='openvino')\n",
    "pipe.vae = torch.compile(pipe.vae.eval(), backend='openvino')\n",
    "\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)  # DDIMScheduler is used because UNet quantization produces better results with it\n",
    "pipe.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide numpy seed for random generation\n",
    "For an equal conversion with the Openvino Stable Diffusion pipeline, the latent for this pipeline is passed manually after randomly generating using numpy random generator which utilizes a random numpy seed like the original OV Stable Diffusion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Downloads/openvino_notebooks/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:188: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n",
      "  return getattr(self._orig_mod, name)\n"
     ]
    }
   ],
   "source": [
    "#Generating latent using numpy seed to make the image reproducible with OV Stable diffusion pipeline\n",
    "np.random.seed(seed)\n",
    "latents = np.random.randn(1, pipe.unet.in_channels, 96, 96).astype(np.float32)\n",
    "latents = torch.from_numpy(latents).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Compilation\n",
    "\n",
    "This step involves passing the data to initially compile all the models in the pipeline after inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1002fac92304718a4ccb135c0987401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Warmup the model for initial compile\n",
    "prompt = \"valley in the Alps at sunset, epic vista, beautiful landscape, 4k, 8k\"\n",
    "negative_prompt = \"frames, borderline, text, charachter, duplicate, error, out of frame, watermark, low quality, ugly, deformed, blur\"\n",
    "num_steps = 1\n",
    "\n",
    "image = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=num_steps, latents=latents).images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Inference\n",
    "Generating an image with the same parameters as the original OV Stable diffusion model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836de5b35efb486b97ae6d378c0f77f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 25\n",
    "\n",
    "with torch.no_grad():\n",
    "    image = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=num_steps, latents=latents, guidance_scale=7.5).images[0]\n",
    "image.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
