{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5501f221fd5d19f",
   "metadata": {},
   "source": [
    "# Text-to-Image Generation with Stable Diffusion v2 and OpenVINO™\n",
    "\n",
    "Stable Diffusion v2 is the next generation of Stable Diffusion model a Text-to-Image latent diffusion model created by the researchers and engineers from [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/). \n",
    "\n",
    "General diffusion models are machine learning systems that are trained to denoise random gaussian noise step by step, to get to a sample of interest, such as an image.\n",
    "Diffusion models have shown to achieve state-of-the-art results for generating image data. But one downside of diffusion models is that the reverse denoising process is slow. In addition, these models consume a lot of memory because they operate in pixel space, which becomes unreasonably expensive when generating high-resolution images. Therefore, it is challenging to train these models and also use them for inference. OpenVINO brings capabilities to run model inference on Intel hardware and opens the door to the fantastic world of diffusion models for everyone!\n",
    "\n",
    "In previous notebooks, we already discussed how to run [Text-to-Image generation and Image-to-Image generation using Stable Diffusion v1](../stable-diffusion-text-to-image/stable-diffusion-text-to-image.ipynb) and [controlling its generation process using ControlNet](./controlnet-stable-diffusion/controlnet-stable-diffusion.ipynb). Now is turn of Stable Diffusion v2.\n",
    "\n",
    "## Stable Diffusion v2: What’s new?\n",
    "\n",
    "The new stable diffusion model offers a bunch of new features inspired by the other models that have emerged since the introduction of the first iteration. Some of the features that can be found in the new model are:\n",
    "\n",
    "* The model comes with a new robust encoder, OpenCLIP, created by LAION and aided by Stability AI; this version v2 significantly enhances the produced photos over the V1 versions. \n",
    "* The model can now generate images in a 768x768 resolution, offering more information to be shown in the generated images.\n",
    "* The model finetuned with [v-objective](https://arxiv.org/abs/2202.00512). The v-parameterization is particularly useful for numerical stability throughout the diffusion process to enable progressive distillation for models. For models that operate at higher resolution, it is also discovered that the v-parameterization avoids color shifting artifacts that are known to affect high resolution diffusion models, and in the video setting it avoids temporal color shifting that sometimes appears with epsilon-prediction used in Stable Diffusion v1. \n",
    "* The model also comes with a new diffusion model capable of running upscaling on the images generated. Upscaled images can be adjusted up to 4 times the original image. Provided as separated model, for more details please check [stable-diffusion-x4-upscaler](https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler)\n",
    "* The model comes with a new refined depth architecture capable of preserving context from prior generation layers in an image-to-image setting. This structure preservation helps generate images that preserving forms and shadow of objects, but with different content.\n",
    "* The model comes with an updated inpainting module built upon the previous model. This text-guided inpainting makes switching out parts in the image easier than before.\n",
    "\n",
    "This notebook demonstrates how to convert and run Stable Diffusion v2 model using OpenVINO.\n",
    "\n",
    "Notebook contains the following steps:\n",
    "\n",
    "1. Create PyTorch models pipeline using Diffusers library.\n",
    "2. Convert PyTorch models to OpenVINO IR format, using model conversion API.\n",
    "3. Apply hybrid post-training quantization to UNet model with [NNCF](https://github.com/openvinotoolkit/nncf/).\n",
    "4. Run Stable Diffusion v2 Text-to-Image pipeline with OpenVINO.\n",
    "\n",
    "**Note:** This is the full version of the Stable Diffusion text-to-image implementation. If you would like to get started and run the notebook quickly, check out [stable-diffusion-v2-text-to-image-demo notebook](../stable-diffusion-v2/stable-diffusion-v2-text-to-image-demo.ipynb).\n",
    "\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/stable-diffusion-v2/stable-diffusion-v2-text-to-image.ipynb\" />\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2bceddde9f8d526",
   "metadata": {},
   "source": [
    "#### Table of contents:\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Stable Diffusion v2 for Text-to-Image Generation](#Stable-Diffusion-v2-for-Text-to-Image-Generation)\n",
    "    - [Stable Diffusion in Diffusers library](#Stable-Diffusion-in-Diffusers-library)\n",
    "    - [Convert models to OpenVINO Intermediate representation (IR) format](#Convert-models-to-OpenVINO-Intermediate-representation-(IR)-format)\n",
    "    - [Text Encoder](#Text-Encoder)\n",
    "    - [U-Net](#U-Net)\n",
    "    - [VAE](#VAE)\n",
    "    - [Prepare Inference Pipeline](#Prepare-Inference-Pipeline)\n",
    "    - [Configure Inference Pipeline](#Configure-Inference-Pipeline)\n",
    "- [Quantization](#Quantization)\n",
    "    - [Prepare calibration dataset](#Prepare-calibration-dataset)\n",
    "    - [Run Hybrid Model Quantization](#Run-Hybrid-Model-Quantization)\n",
    "    - [Compare inference time of the FP16 and INT8 pipelines](#Compare-inference-time-of-the-FP16-and-INT8-pipelines)\n",
    "- [Run Text-to-Image generation](#Run-Text-to-Image-generation)\n",
    "\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a571d16e81bf3c4",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6723fa8e346926b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T13:29:10.765014Z",
     "start_time": "2024-02-13T13:29:08.472566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"diffusers>=0.14.0\" openvino-nightly \"datasets>=2.14.6\" \"transformers>=4.25.1\" \"gradio>=4.19\" \"torch>=2.1\" Pillow opencv-python --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q git+https://github.com/openvinotoolkit/nncf.git\n",
    "%pip install -q accelerate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c4a678aa7817277",
   "metadata": {},
   "source": [
    "## Stable Diffusion v2 for Text-to-Image Generation\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "To start, let's look on Text-to-Image process for Stable Diffusion v2. We will use [Stable Diffusion v2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1) model for these purposes. The main difference from Stable Diffusion v2 and Stable Diffusion v2.1 is usage of more data, more training, and less restrictive filtering of the dataset, that gives promising results for selecting wide range of input text prompts. More details about model can be found in [Stability AI blog post](https://stability.ai/blog/stablediffusion2-1-release7-dec-2022) and original model [repository](https://github.com/Stability-AI/stablediffusion).\n",
    "\n",
    "### Stable Diffusion in Diffusers library\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "To work with Stable Diffusion v2, we will use Hugging Face [Diffusers](https://github.com/huggingface/diffusers) library. To experiment with Stable Diffusion models, Diffusers exposes the [`StableDiffusionPipeline`](https://huggingface.co/docs/diffusers/using-diffusers/conditional_image_generation) similar to the [other Diffusers pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview).  The code below demonstrates how to create `StableDiffusionPipeline` using `stable-diffusion-2-1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5988a7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25114d5b9064f2788dca843ccc74ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Downloads/openvino_notebooks/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "from nncf.torch.dynamic_graph.patch_pytorch import disable_patching  # for optimal performance of quantized model\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\").to(\"cpu\")\n",
    "\n",
    "text_encoder = pipe.text_encoder.eval()\n",
    "unet = pipe.unet.eval()\n",
    "vae = pipe.vae.eval()\n",
    "\n",
    "conf = pipe.scheduler.config\n",
    "del pipe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a636f5007c3781e",
   "metadata": {},
   "source": [
    "### Convert models to OpenVINO Intermediate representation (IR) format\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Starting from 2023.0 release, OpenVINO supports PyTorch models directly via Model Conversion API. `ov.convert_model` function accepts instance of PyTorch model and example inputs for tracing and returns object of `ov.Model` class, ready to use or save on disk using `ov.save_model` function. \n",
    "\n",
    "\n",
    "The pipeline consists of three important parts:\n",
    "\n",
    "* Text Encoder to create condition to generate an image from a text prompt.\n",
    "* U-Net for step-by-step denoising latent image representation.\n",
    "* Autoencoder (VAE) for decoding latent space to image.\n",
    "\n",
    "Let us convert each part:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4f14633b43a9034",
   "metadata": {},
   "source": [
    "### Text Encoder\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The text-encoder is responsible for transforming the input prompt, for example, \"a photo of an astronaut riding a horse\" into an embedding space that can be understood by the U-Net. It is usually a simple transformer-based encoder that maps a sequence of input tokens to a sequence of latent text embeddings.\n",
    "\n",
    "The input of the text encoder is tensor `input_ids`, which contains indexes of tokens from text processed by the tokenizer and padded to the maximum length accepted by the model. Model outputs are two tensors: `last_hidden_state` - hidden state from the last MultiHeadAttention layer in the model and `pooler_out` - pooled output for whole model hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd5f7ff0751a146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T13:29:13.434571Z",
     "start_time": "2024-02-13T13:29:13.432325Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "sd2_1_model_dir = Path(\"sd2.1\")\n",
    "sd2_1_model_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19a36c5ca10a998a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T13:29:20.605486Z",
     "start_time": "2024-02-13T13:29:13.435752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoder will be loaded from sd2.1/text_encoder.xml\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "import openvino as ov\n",
    "\n",
    "TEXT_ENCODER_OV_PATH = sd2_1_model_dir / \"text_encoder.xml\"\n",
    "\n",
    "\n",
    "def cleanup_torchscript_cache():\n",
    "    \"\"\"\n",
    "    Helper for removing cached model representation\n",
    "    \"\"\"\n",
    "    torch._C._jit_clear_class_registry()\n",
    "    torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n",
    "    torch.jit._state._clear_class_state()\n",
    "\n",
    "\n",
    "def convert_encoder(text_encoder: torch.nn.Module, ir_path: Path):\n",
    "    \"\"\"\n",
    "    Convert Text Encoder model to IR.\n",
    "    Function accepts pipeline, prepares example inputs for conversion\n",
    "    Parameters:\n",
    "        text_encoder (torch.nn.Module): text encoder PyTorch model\n",
    "        ir_path (Path): File for storing model\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not ir_path.exists():\n",
    "        input_ids = torch.ones((1, 77), dtype=torch.long)\n",
    "        # switch model to inference mode\n",
    "        text_encoder.eval()\n",
    "\n",
    "        # disable gradients calculation for reducing memory consumption\n",
    "        with torch.no_grad():\n",
    "            # export model\n",
    "            ov_model = ov.convert_model(\n",
    "                text_encoder,  # model instance\n",
    "                example_input=input_ids,  # example inputs for model tracing\n",
    "                input=([1, 77],),  # input shape for conversion\n",
    "            )\n",
    "            ov.save_model(ov_model, ir_path)\n",
    "            del ov_model\n",
    "            cleanup_torchscript_cache()\n",
    "        print(\"Text Encoder successfully converted to IR\")\n",
    "\n",
    "\n",
    "if not TEXT_ENCODER_OV_PATH.exists():\n",
    "    convert_encoder(text_encoder, TEXT_ENCODER_OV_PATH)\n",
    "else:\n",
    "    print(f\"Text encoder will be loaded from {TEXT_ENCODER_OV_PATH}\")\n",
    "\n",
    "del text_encoder\n",
    "gc.collect();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccaba8325efa1177",
   "metadata": {},
   "source": [
    "### U-Net\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "U-Net model gradually denoises latent image representation guided by text encoder hidden state.\n",
    "\n",
    "U-Net model has three inputs:\n",
    "\n",
    "* `sample` - latent image sample from previous step. Generation process has not been started yet, so you will use random noise.\n",
    "* `timestep` - current scheduler step.\n",
    "* `encoder_hidden_state` - hidden state of text encoder.\n",
    "\n",
    "Model predicts the `sample` state for the next step.\n",
    "\n",
    "Generally, U-Net model conversion process remain the same like in Stable Diffusion v1, expect small changes in input sample size.  Our model was pretrained to generate images with resolution 768x768, initial latent sample size for this case is 96x96. Besides that, for different use cases like inpainting and depth to image generation model also can accept additional image information: depth map or mask as channel-wise concatenation with initial latent sample. For converting U-Net model for such use cases required to modify number of input channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ee42bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch._export import capture_pre_autograd_graph\n",
    "\n",
    "\n",
    "def convert_fx_unet(\n",
    "    unet: torch.fx.GraphModule,\n",
    "    num_channels: int = 4,\n",
    "    width: int = 96,\n",
    "    height: int = 96,\n",
    "):\n",
    "    # prepare inputs\n",
    "    encoder_hidden_state = torch.ones((2, 77, 1024))\n",
    "    latents_shape = (2, num_channels, width, height)\n",
    "    latents = torch.randn(latents_shape)\n",
    "    t = torch.from_numpy(np.array(1, dtype=np.float32))\n",
    "    unet.eval()\n",
    "    dummy_inputs = (latents, t, encoder_hidden_state)\n",
    "    with disable_patching():\n",
    "        with torch.no_grad():\n",
    "            fx_unet = capture_pre_autograd_graph(unet, args=dummy_inputs)\n",
    "    print(\"U-Net successfully converted to FX\")\n",
    "    del unet\n",
    "    cleanup_torchscript_cache()\n",
    "    return fx_unet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e52e6090fd6a87b3",
   "metadata": {},
   "source": [
    "### VAE\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The VAE model has two parts, an encoder and a decoder. The encoder is used to convert the image into a low dimensional latent representation, which will serve as the input to the U-Net model. The decoder, conversely, transforms the latent representation back into an image.\n",
    "\n",
    "During latent diffusion training, the encoder is used to get the latent representations (latents) of the images for the forward diffusion process, which applies more and more noise at each step. During inference, the denoised latents generated by the reverse diffusion process are converted back into images using the VAE decoder. When you run inference for Text-to-Image, there is no initial image as a starting point. You can skip this step and directly generate initial random noise.\n",
    "\n",
    "When running Text-to-Image pipeline, we will see that we **only need the VAE decoder**, but preserve VAE encoder conversion, it will be useful in next chapter of our tutorial. \n",
    "\n",
    "Note: This process will take a few minutes and use significant amount of RAM (recommended at least 32GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "442461d2f19e8c99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T13:30:16.030912Z",
     "start_time": "2024-02-13T13:29:44.431075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE encoder will be loaded from sd2.1/vae_encoder.xml\n",
      "VAE decoder will be loaded from sd2.1/vae_decoder.xml\n"
     ]
    }
   ],
   "source": [
    "VAE_ENCODER_OV_PATH = sd2_1_model_dir / \"vae_encoder.xml\"\n",
    "\n",
    "\n",
    "def convert_vae_encoder(vae: torch.nn.Module, ir_path: Path, width: int = 512, height: int = 512):\n",
    "    \"\"\"\n",
    "    Convert VAE model to IR format.\n",
    "    VAE model, creates wrapper class for export only necessary for inference part,\n",
    "    prepares example inputs for onversion\n",
    "    Parameters:\n",
    "        vae (torch.nn.Module): VAE PyTorch model\n",
    "        ir_path (Path): File for storing model\n",
    "        width (int, optional, 512): input width\n",
    "        height (int, optional, 512): input height\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    class VAEEncoderWrapper(torch.nn.Module):\n",
    "        def __init__(self, vae):\n",
    "            super().__init__()\n",
    "            self.vae = vae\n",
    "\n",
    "        def forward(self, image):\n",
    "            return self.vae.encode(x=image)[\"latent_dist\"].sample()\n",
    "\n",
    "    if not ir_path.exists():\n",
    "        vae_encoder = VAEEncoderWrapper(vae)\n",
    "        vae_encoder.eval()\n",
    "        image = torch.zeros((1, 3, width, height))\n",
    "        with torch.no_grad():\n",
    "            ov_model = ov.convert_model(vae_encoder, example_input=image, input=([1, 3, width, height],))\n",
    "        ov.save_model(ov_model, ir_path)\n",
    "        del ov_model\n",
    "        cleanup_torchscript_cache()\n",
    "        print(\"VAE encoder successfully converted to IR\")\n",
    "\n",
    "\n",
    "def convert_vae_decoder(vae: torch.nn.Module, ir_path: Path, width: int = 64, height: int = 64):\n",
    "    \"\"\"\n",
    "    Convert VAE decoder model to IR format.\n",
    "    Function accepts VAE model, creates wrapper class for export only necessary for inference part,\n",
    "    prepares example inputs for conversion\n",
    "    Parameters:\n",
    "        vae (torch.nn.Module): VAE model\n",
    "        ir_path (Path): File for storing model\n",
    "        width (int, optional, 64): input width\n",
    "        height (int, optional, 64): input height\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    class VAEDecoderWrapper(torch.nn.Module):\n",
    "        def __init__(self, vae):\n",
    "            super().__init__()\n",
    "            self.vae = vae\n",
    "\n",
    "        def forward(self, latents):\n",
    "            return self.vae.decode(latents)\n",
    "\n",
    "    if not ir_path.exists():\n",
    "        vae_decoder = VAEDecoderWrapper(vae)\n",
    "        latents = torch.zeros((1, 4, width, height))\n",
    "\n",
    "        vae_decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            ov_model = ov.convert_model(vae_decoder, example_input=latents, input=([1, 4, width, height],))\n",
    "        ov.save_model(ov_model, ir_path)\n",
    "        del ov_model\n",
    "        cleanup_torchscript_cache()\n",
    "        print(\"VAE decoder successfully converted to IR\")\n",
    "\n",
    "\n",
    "if not VAE_ENCODER_OV_PATH.exists():\n",
    "    convert_vae_encoder(vae, VAE_ENCODER_OV_PATH, 768, 768)\n",
    "else:\n",
    "    print(f\"VAE encoder will be loaded from {VAE_ENCODER_OV_PATH}\")\n",
    "\n",
    "VAE_DECODER_OV_PATH = sd2_1_model_dir / \"vae_decoder.xml\"\n",
    "\n",
    "if not VAE_DECODER_OV_PATH.exists():\n",
    "    convert_vae_decoder(vae, VAE_DECODER_OV_PATH, 96, 96)\n",
    "else:\n",
    "    print(f\"VAE decoder will be loaded from {VAE_DECODER_OV_PATH}\")\n",
    "\n",
    "del vae\n",
    "gc.collect();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2009916bbc6c3ec8",
   "metadata": {},
   "source": [
    "### Prepare Inference Pipeline\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Putting it all together, let us now take a closer look at how the model works in inference by illustrating the logical flow.\n",
    "\n",
    "![text2img-stable-diffusion v2](https://github.com/openvinotoolkit/openvino_notebooks/assets/22090501/ec454103-0d28-48e3-a18e-b55da3fab381)\n",
    "\n",
    "The stable diffusion model takes both a latent seed and a text prompt as input. The latent seed is then used to generate random latent image representations of size $96 \\times 96$ where as the text prompt is transformed to text embeddings of size $77 \\times 1024$ via OpenCLIP's text encoder.\n",
    "\n",
    "Next, the U-Net iteratively *denoises* the random latent image representations while being conditioned on the text embeddings. The output of the U-Net, being the noise residual, is used to compute a denoised latent image representation via a scheduler algorithm. Many different scheduler algorithms can be used for this computation, each having its pros and cons. For Stable Diffusion, it is recommended to use one of:\n",
    "\n",
    "- [PNDM scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py)\n",
    "- [DDIM scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py)\n",
    "- [K-LMS scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py)\n",
    "\n",
    "Theory on how the scheduler algorithm function works is out of scope for this notebook, but in short, you should remember that they compute the predicted denoised image representation from the previous noise representation and the predicted noise residual.\n",
    "For more information, it is recommended to look into [Elucidating the Design Space of Diffusion-Based Generative Models](https://arxiv.org/abs/2206.00364).\n",
    "\n",
    "\n",
    "The chart above looks very similar to Stable Diffusion V1 from [notebook](../stable-diffusion-text-to-image/stable-diffusion-text-to-image.ipynb), but there is some small difference in details:\n",
    "\n",
    "* Changed input resolution for U-Net model.\n",
    "* Changed text encoder and as the result size of its hidden state embeddings.\n",
    "* Additionally, to improve image generation quality authors introduced negative prompting. Technically, positive prompt steers the diffusion toward the images associated with it, while negative prompt steers the diffusion away from it.In other words, negative prompt declares undesired concepts for generation image, e.g. if we want to have colorful and bright image, gray scale image will be result which we want to avoid, in this case gray scale can be treated as negative prompt. The positive and negative prompt are in equal footing. You can always use one with or without the other. More explanation of how it works can be found in this [article](https://stable-diffusion-art.com/how-negative-prompt-work/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba60aaf6be593f39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T13:30:16.082794Z",
     "start_time": "2024-02-13T13:30:16.032084Z"
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "from typing import List, Optional, Union, Dict\n",
    "\n",
    "import PIL\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "from transformers import CLIPTokenizer\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers.schedulers import DDIMScheduler, LMSDiscreteScheduler, PNDMScheduler\n",
    "\n",
    "\n",
    "def scale_fit_to_window(dst_width: int, dst_height: int, image_width: int, image_height: int):\n",
    "    \"\"\"\n",
    "    Preprocessing helper function for calculating image size for resize with peserving original aspect ratio\n",
    "    and fitting image to specific window size\n",
    "\n",
    "    Parameters:\n",
    "      dst_width (int): destination window width\n",
    "      dst_height (int): destination window height\n",
    "      image_width (int): source image width\n",
    "      image_height (int): source image height\n",
    "    Returns:\n",
    "      result_width (int): calculated width for resize\n",
    "      result_height (int): calculated height for resize\n",
    "    \"\"\"\n",
    "    im_scale = min(dst_height / image_height, dst_width / image_width)\n",
    "    return int(im_scale * image_width), int(im_scale * image_height)\n",
    "\n",
    "\n",
    "def preprocess(image: PIL.Image.Image):\n",
    "    \"\"\"\n",
    "    Image preprocessing function. Takes image in PIL.Image format, resizes it to keep aspect ration and fits to model input window 512x512,\n",
    "    then converts it to np.ndarray and adds padding with zeros on right or bottom side of image (depends from aspect ratio), after that\n",
    "    converts data to float32 data type and change range of values from [0, 255] to [-1, 1], finally, converts data layout from planar NHWC to NCHW.\n",
    "    The function returns preprocessed input tensor and padding size, which can be used in postprocessing.\n",
    "\n",
    "    Parameters:\n",
    "      image (PIL.Image.Image): input image\n",
    "    Returns:\n",
    "       image (np.ndarray): preprocessed image tensor\n",
    "       meta (Dict): dictionary with preprocessing metadata info\n",
    "    \"\"\"\n",
    "    src_width, src_height = image.size\n",
    "    dst_width, dst_height = scale_fit_to_window(512, 512, src_width, src_height)\n",
    "    image = np.array(image.resize((dst_width, dst_height), resample=PIL.Image.Resampling.LANCZOS))[None, :]\n",
    "    pad_width = 512 - dst_width\n",
    "    pad_height = 512 - dst_height\n",
    "    pad = ((0, 0), (0, pad_height), (0, pad_width), (0, 0))\n",
    "    image = np.pad(image, pad, mode=\"constant\")\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    image = 2.0 * image - 1.0\n",
    "    image = image.transpose(0, 3, 1, 2)\n",
    "    return image, {\"padding\": pad, \"src_width\": src_width, \"src_height\": src_height}\n",
    "\n",
    "\n",
    "class OVStableDiffusionPipeline(DiffusionPipeline):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vae_decoder: ov.Model,\n",
    "        text_encoder: ov.Model,\n",
    "        tokenizer: CLIPTokenizer,\n",
    "        unet: ov.Model,\n",
    "        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n",
    "        vae_encoder: ov.Model = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Pipeline for text-to-image generation using Stable Diffusion.\n",
    "        Parameters:\n",
    "            vae_decoder (Model):\n",
    "                Variational Auto-Encoder (VAE) Model to decode images to and from latent representations.\n",
    "            text_encoder (Model):\n",
    "                Frozen text-encoder. Stable Diffusion uses the text portion of\n",
    "                [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n",
    "                the clip-vit-large-patch14(https://huggingface.co/openai/clip-vit-large-patch14) variant.\n",
    "            tokenizer (CLIPTokenizer):\n",
    "                Tokenizer of class CLIPTokenizer(https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n",
    "            unet (Model): Conditional U-Net architecture to denoise the encoded image latents.\n",
    "            vae_encoder (Model):\n",
    "                Variational Auto-Encoder (VAE) Model to encode images to latent representation.\n",
    "            scheduler (SchedulerMixin):\n",
    "                A scheduler to be used in combination with unet to denoise the encoded image latents. Can be one of\n",
    "                DDIMScheduler, LMSDiscreteScheduler, or PNDMScheduler.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.scheduler = scheduler\n",
    "        self.vae_decoder = vae_decoder\n",
    "        self.vae_encoder = vae_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.unet = unet\n",
    "        self.register_to_config(unet=unet)\n",
    "        self._text_encoder_output = text_encoder.output(0)\n",
    "        self._vae_d_output = vae_decoder.output(0)\n",
    "        self._vae_e_output = vae_encoder.output(0) if vae_encoder is not None else None\n",
    "        self.height = self._get_input_shape(unet).shape[2] * 8\n",
    "        self.width = self._get_input_shape(unet).shape[3] * 8\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]],\n",
    "        image: PIL.Image.Image = None,\n",
    "        negative_prompt: Union[str, List[str]] = None,\n",
    "        num_inference_steps: Optional[int] = 50,\n",
    "        guidance_scale: Optional[float] = 7.5,\n",
    "        eta: Optional[float] = 0.0,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        seed: Optional[int] = None,\n",
    "        strength: float = 1.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Function invoked when calling the pipeline for generation.\n",
    "        Parameters:\n",
    "            prompt (str or List[str]):\n",
    "                The prompt or prompts to guide the image generation.\n",
    "            image (PIL.Image.Image, *optional*, None):\n",
    "                 Intinal image for generation.\n",
    "            negative_prompt (str or List[str]):\n",
    "                The negative prompt or prompts to guide the image generation.\n",
    "            num_inference_steps (int, *optional*, defaults to 50):\n",
    "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
    "                expense of slower inference.\n",
    "            guidance_scale (float, *optional*, defaults to 7.5):\n",
    "                Guidance scale as defined in Classifier-Free Diffusion Guidance(https://arxiv.org/abs/2207.12598).\n",
    "                guidance_scale is defined as `w` of equation 2.\n",
    "                Higher guidance scale encourages to generate images that are closely linked to the text prompt,\n",
    "                usually at the expense of lower image quality.\n",
    "            eta (float, *optional*, defaults to 0.0):\n",
    "                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n",
    "                [DDIMScheduler], will be ignored for others.\n",
    "            output_type (`str`, *optional*, defaults to \"pil\"):\n",
    "                The output format of the generate image. Choose between\n",
    "                [PIL](https://pillow.readthedocs.io/en/stable/): PIL.Image.Image or np.array.\n",
    "            seed (int, *optional*, None):\n",
    "                Seed for random generator state initialization.\n",
    "            strength (int, *optional*, 1.0):\n",
    "                strength between initial image and generated in Image-to-Image pipeline, do not used in Text-to-Image\n",
    "        Returns:\n",
    "            Dictionary with keys:\n",
    "                sample - the last generated image PIL.Image.Image or np.array\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
    "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
    "        # corresponds to doing no classifier free guidance.\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "        # get prompt text embeddings\n",
    "        text_embeddings = self._encode_prompt(\n",
    "            prompt,\n",
    "            do_classifier_free_guidance=do_classifier_free_guidance,\n",
    "            negative_prompt=negative_prompt,\n",
    "        )\n",
    "        # set timesteps\n",
    "        accepts_offset = \"offset\" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())\n",
    "        extra_set_kwargs = {}\n",
    "        if accepts_offset:\n",
    "            extra_set_kwargs[\"offset\"] = 1\n",
    "\n",
    "        self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
    "        timesteps, num_inference_steps = self.get_timesteps(num_inference_steps, strength)\n",
    "        latent_timestep = timesteps[:1]\n",
    "\n",
    "        # get the initial random noise unless the user supplied it\n",
    "        latents, meta = self.prepare_latents(image, latent_timestep)\n",
    "\n",
    "        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
    "        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
    "        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n",
    "        # and should be between [0, 1]\n",
    "        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
    "        extra_step_kwargs = {}\n",
    "        if accepts_eta:\n",
    "            extra_step_kwargs[\"eta\"] = eta\n",
    "\n",
    "        for t in self.progress_bar(timesteps):\n",
    "            # expand the latents if we are doing classifier free guidance\n",
    "            latent_model_input = np.concatenate([latents] * 2) if do_classifier_free_guidance else latents\n",
    "            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "            # predict the noise residual\n",
    "            noise_pred = self.unet(torch.from_numpy(latent_model_input), t, torch.from_numpy(text_embeddings))\n",
    "            # perform guidance\n",
    "            if do_classifier_free_guidance:\n",
    "                noise_pred_uncond, noise_pred_text = noise_pred[0][0], noise_pred[0][1]\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            latents = self.scheduler.step(noise_pred, t, torch.from_numpy(latents), **extra_step_kwargs)[\"prev_sample\"].detach().numpy()\n",
    "        # scale and decode the image latents with vae\n",
    "        image = self.vae_decoder(latents * (1 / 0.18215))[self._vae_d_output]\n",
    "\n",
    "        image = self.postprocess_image(image, meta, output_type)\n",
    "        return {\"sample\": image}\n",
    "\n",
    "    def _get_input_shape(self, model: torch.fx.GraphModule):\n",
    "        return list(model.graph.nodes)[0].meta[\"val\"]\n",
    "\n",
    "    def _encode_prompt(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]],\n",
    "        num_images_per_prompt: int = 1,\n",
    "        do_classifier_free_guidance: bool = True,\n",
    "        negative_prompt: Union[str, List[str]] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Encodes the prompt into text encoder hidden states.\n",
    "\n",
    "        Parameters:\n",
    "            prompt (str or list(str)): prompt to be encoded\n",
    "            num_images_per_prompt (int): number of images that should be generated per prompt\n",
    "            do_classifier_free_guidance (bool): whether to use classifier free guidance or not\n",
    "            negative_prompt (str or list(str)): negative prompt to be encoded\n",
    "        Returns:\n",
    "            text_embeddings (np.ndarray): text encoder hidden states\n",
    "        \"\"\"\n",
    "        batch_size = len(prompt) if isinstance(prompt, list) else 1\n",
    "\n",
    "        # tokenize input prompts\n",
    "        text_inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"np\",\n",
    "        )\n",
    "        text_input_ids = text_inputs.input_ids\n",
    "\n",
    "        text_embeddings = self.text_encoder(text_input_ids)[self._text_encoder_output]\n",
    "\n",
    "        # duplicate text embeddings for each generation per prompt\n",
    "        if num_images_per_prompt != 1:\n",
    "            bs_embed, seq_len, _ = text_embeddings.shape\n",
    "            text_embeddings = np.tile(text_embeddings, (1, num_images_per_prompt, 1))\n",
    "            text_embeddings = np.reshape(text_embeddings, (bs_embed * num_images_per_prompt, seq_len, -1))\n",
    "\n",
    "        # get unconditional embeddings for classifier free guidance\n",
    "        if do_classifier_free_guidance:\n",
    "            uncond_tokens: List[str]\n",
    "            max_length = text_input_ids.shape[-1]\n",
    "            if negative_prompt is None:\n",
    "                uncond_tokens = [\"\"] * batch_size\n",
    "            elif isinstance(negative_prompt, str):\n",
    "                uncond_tokens = [negative_prompt]\n",
    "            else:\n",
    "                uncond_tokens = negative_prompt\n",
    "            uncond_input = self.tokenizer(\n",
    "                uncond_tokens,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"np\",\n",
    "            )\n",
    "\n",
    "            uncond_embeddings = self.text_encoder(uncond_input.input_ids)[self._text_encoder_output]\n",
    "\n",
    "            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n",
    "            seq_len = uncond_embeddings.shape[1]\n",
    "            uncond_embeddings = np.tile(uncond_embeddings, (1, num_images_per_prompt, 1))\n",
    "            uncond_embeddings = np.reshape(uncond_embeddings, (batch_size * num_images_per_prompt, seq_len, -1))\n",
    "\n",
    "            # For classifier free guidance, we need to do two forward passes.\n",
    "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
    "            # to avoid doing two forward passes\n",
    "            text_embeddings = np.concatenate([uncond_embeddings, text_embeddings])\n",
    "\n",
    "        return text_embeddings\n",
    "\n",
    "    def prepare_latents(self, image: PIL.Image.Image = None, latent_timestep: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Function for getting initial latents for starting generation\n",
    "\n",
    "        Parameters:\n",
    "            image (PIL.Image.Image, *optional*, None):\n",
    "                Input image for generation, if not provided randon noise will be used as starting point\n",
    "            latent_timestep (torch.Tensor, *optional*, None):\n",
    "                Predicted by scheduler initial step for image generation, required for latent image mixing with nosie\n",
    "        Returns:\n",
    "            latents (np.ndarray):\n",
    "                Image encoded in latent space\n",
    "        \"\"\"\n",
    "        latents_shape = (1, 4, self.height // 8, self.width // 8)\n",
    "        noise = np.random.randn(*latents_shape).astype(np.float32)\n",
    "        if image is None:\n",
    "            # if we use LMSDiscreteScheduler, let's make sure latents are mulitplied by sigmas\n",
    "            if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
    "                noise = noise * self.scheduler.sigmas[0].numpy()\n",
    "            return noise, {}\n",
    "        input_image, meta = preprocess(image)\n",
    "        latents = self.vae_encoder(input_image)[self._vae_e_output]\n",
    "        latents = latents * 0.18215\n",
    "        latents = self.scheduler.add_noise(torch.from_numpy(latents), torch.from_numpy(noise), latent_timestep).numpy()\n",
    "        return latents, meta\n",
    "\n",
    "    def postprocess_image(self, image: np.ndarray, meta: Dict, output_type: str = \"pil\"):\n",
    "        \"\"\"\n",
    "        Postprocessing for decoded image. Takes generated image decoded by VAE decoder, unpad it to initila image size (if required),\n",
    "        normalize and convert to [0, 255] pixels range. Optionally, convertes it from np.ndarray to PIL.Image format\n",
    "\n",
    "        Parameters:\n",
    "            image (np.ndarray):\n",
    "                Generated image\n",
    "            meta (Dict):\n",
    "                Metadata obtained on latents preparing step, can be empty\n",
    "            output_type (str, *optional*, pil):\n",
    "                Output format for result, can be pil or numpy\n",
    "        Returns:\n",
    "            image (List of np.ndarray or PIL.Image.Image):\n",
    "                Postprocessed images\n",
    "        \"\"\"\n",
    "        if \"padding\" in meta:\n",
    "            pad = meta[\"padding\"]\n",
    "            (_, end_h), (_, end_w) = pad[1:3]\n",
    "            h, w = image.shape[2:]\n",
    "            unpad_h = h - end_h\n",
    "            unpad_w = w - end_w\n",
    "            image = image[:, :, :unpad_h, :unpad_w]\n",
    "        image = np.clip(image / 2 + 0.5, 0, 1)\n",
    "        image = np.transpose(image, (0, 2, 3, 1))\n",
    "        # 9. Convert to PIL\n",
    "        if output_type == \"pil\":\n",
    "            image = self.numpy_to_pil(image)\n",
    "            if \"src_height\" in meta:\n",
    "                orig_height, orig_width = meta[\"src_height\"], meta[\"src_width\"]\n",
    "                image = [img.resize((orig_width, orig_height), PIL.Image.Resampling.LANCZOS) for img in image]\n",
    "        else:\n",
    "            if \"src_height\" in meta:\n",
    "                orig_height, orig_width = meta[\"src_height\"], meta[\"src_width\"]\n",
    "                image = [cv2.resize(img, (orig_width, orig_width)) for img in image]\n",
    "        return image\n",
    "\n",
    "    def get_timesteps(self, num_inference_steps: int, strength: float):\n",
    "        \"\"\"\n",
    "        Helper function for getting scheduler timesteps for generation\n",
    "        In case of image-to-image generation, it updates number of steps according to strength\n",
    "\n",
    "        Parameters:\n",
    "           num_inference_steps (int):\n",
    "              number of inference steps for generation\n",
    "           strength (float):\n",
    "               value between 0.0 and 1.0, that controls the amount of noise that is added to the input image.\n",
    "               Values that approach 1.0 allow for lots of variations but will also produce images that are not semantically consistent with the input.\n",
    "        \"\"\"\n",
    "        # get the original timestep using init_timestep\n",
    "        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n",
    "\n",
    "        t_start = max(num_inference_steps - init_timestep, 0)\n",
    "        timesteps = self.scheduler.timesteps[t_start:]\n",
    "\n",
    "        return timesteps, num_inference_steps - t_start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ae7b18e9f8c995a",
   "metadata": {},
   "source": [
    "### Configure Inference Pipeline\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "First, you should create instances of OpenVINO Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "635200a7b5d84bf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T13:30:16.325848Z",
     "start_time": "2024-02-13T13:30:16.083685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab23a392f4e04b658690396c481113e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=1, options=('CPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab277c23095fa5a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T13:30:19.234103Z",
     "start_time": "2024-02-13T13:30:16.326662Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0823 13:29:08.098000 138791512183232 torch/_export/__init__.py:95] +============================+\n",
      "W0823 13:29:08.099000 138791512183232 torch/_export/__init__.py:96] |     !!!   WARNING   !!!    |\n",
      "W0823 13:29:08.099000 138791512183232 torch/_export/__init__.py:97] +============================+\n",
      "W0823 13:29:08.100000 138791512183232 torch/_export/__init__.py:98] capture_pre_autograd_graph() is deprecated and doesn't provide any function guarantee moving forward.\n",
      "W0823 13:29:08.100000 138791512183232 torch/_export/__init__.py:99] Please switch to use torch.export instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U-Net successfully converted to FX\n"
     ]
    }
   ],
   "source": [
    "ov_config = {\"INFERENCE_PRECISION_HINT\": \"f32\"} if device.value != \"CPU\" else {}\n",
    "\n",
    "text_enc = core.compile_model(TEXT_ENCODER_OV_PATH, device.value)\n",
    "unet_fx_model = convert_fx_unet(unet).cpu()\n",
    "vae_decoder = core.compile_model(VAE_DECODER_OV_PATH, device.value, ov_config)\n",
    "vae_encoder = core.compile_model(VAE_ENCODER_OV_PATH, device.value, ov_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0cdcdb5a1c10f1a",
   "metadata": {},
   "source": [
    "Model tokenizer and scheduler are also important parts of the pipeline. Let us define them and put all components together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25897d22da482d91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T13:30:19.432106Z",
     "start_time": "2024-02-13T13:30:19.236629Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Downloads/openvino_notebooks/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTokenizer\n",
    "\n",
    "scheduler = DDIMScheduler.from_config(conf)  # DDIMScheduler is used because UNet quantization produces better results with it\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "ov_pipe = OVStableDiffusionPipeline(\n",
    "    tokenizer=tokenizer,\n",
    "    text_encoder=text_enc,\n",
    "    unet=unet_fx_model,\n",
    "    vae_encoder=vae_encoder,\n",
    "    vae_decoder=vae_decoder,\n",
    "    scheduler=scheduler,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a5f91dfea77c91",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "[NNCF](https://github.com/openvinotoolkit/nncf/) enables post-training quantization by adding quantization layers into model graph and then using a subset of the training dataset to initialize the parameters of these additional quantization layers. Quantized operations are executed in `INT8` instead of `FP32`/`FP16` making model inference faster.\n",
    "\n",
    "According to `Stable Diffusion v2` structure, the UNet model takes up significant portion of the overall pipeline execution time. Now we will show you how to optimize the UNet part using [NNCF](https://github.com/openvinotoolkit/nncf/) to reduce computation cost and speed up the pipeline. Quantizing the rest of the pipeline does not significantly improve inference performance but can lead to a substantial degradation of accuracy.\n",
    "\n",
    "For this model we apply quantization in hybrid mode which means that we quantize: (1) weights of MatMul and Embedding layers and (2) activations of other layers. The steps are the following:\n",
    "\n",
    "1. Create a calibration dataset for quantization.\n",
    "2. Collect operations with weights.\n",
    "3. Run `nncf.compress_model()` to compress only the model weights.\n",
    "4. Run `nncf.quantize()` on the compressed model with weighted operations ignored by providing `ignored_scope` parameter.\n",
    "5. Save the `INT8` model using `openvino.save_model()` function.\n",
    "\n",
    "Please select below whether you would like to run quantization to improve model inference speed.\n",
    "\n",
    "> **NOTE**: Quantization is time and memory consuming operation. Running quantization code below may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5850cecdf32cb56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T13:30:19.437075Z",
     "start_time": "2024-02-13T13:30:19.433071Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9c8392fb7f4613b43a287207ddd1c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Quantization')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_quantize = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Quantization\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "to_quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f9846bbc13089cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T13:30:19.441469Z",
     "start_time": "2024-02-13T13:30:19.437809Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fetch `skip_kernel_extension` module\n",
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/skip_kernel_extension.py\",\n",
    ")\n",
    "open(\"skip_kernel_extension.py\", \"w\").write(r.text)\n",
    "\n",
    "int8_ov_pipe = None\n",
    "\n",
    "%load_ext skip_kernel_extension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75734990a15dcf4b",
   "metadata": {},
   "source": [
    "### Prepare calibration dataset\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "We use a portion of [conceptual_captions](https://huggingface.co/datasets/google-research-datasets/conceptual_captions) dataset from Hugging Face as calibration data.\n",
    "To collect intermediate model inputs for calibration we should customize `CompiledModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c994c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Any, Dict, List\n",
    "import torch\n",
    "\n",
    "def disable_progress_bar(pipeline, disable=True):\n",
    "    if not hasattr(pipeline, \"_progress_bar_config\"):\n",
    "        pipeline._progress_bar_config = {'disable': disable}\n",
    "    else:\n",
    "        pipeline._progress_bar_config['disable'] = disable\n",
    "\n",
    "\n",
    "class UNetWrapper(torch.nn.Module):\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.captured_args = []\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        if np.random.rand() <= 0.7:\n",
    "            self.captured_args.append(args)\n",
    "        return self.unet(*args, **kwargs)\n",
    "\n",
    "def collect_calibration_data(ov_pipe, calibration_dataset_size: int, num_inference_steps: int) -> List[Dict]:\n",
    "    \n",
    "    original_unet = ov_pipe.unet\n",
    "    calibration_data = []\n",
    "    disable_progress_bar(ov_pipe)\n",
    "\n",
    "    dataset = datasets.load_dataset(\"google-research-datasets/conceptual_captions\", split=\"train\", trust_remote_code=True).shuffle(seed=42)\n",
    "\n",
    "    wrapped_unet = UNetWrapper(ov_pipe.unet)\n",
    "    ov_pipe.unet = wrapped_unet\n",
    "\n",
    "    # Run inference for data collection\n",
    "    pbar = tqdm(total=calibration_dataset_size)\n",
    "    for batch in dataset:\n",
    "        prompt = batch[\"caption\"]\n",
    "        if len(prompt) > ov_pipe.tokenizer.model_max_length:\n",
    "            continue\n",
    "        # Run the pipeline\n",
    "        ov_pipe(prompt, num_inference_steps=num_inference_steps, seed=1)\n",
    "        calibration_data.extend(wrapped_unet.captured_args)\n",
    "        wrapped_unet.captured_args = []\n",
    "        pbar.update(len(calibration_data) - pbar.n)\n",
    "        if pbar.n >= calibration_dataset_size:\n",
    "            break\n",
    "\n",
    "    disable_progress_bar(ov_pipe, disable=False)\n",
    "    ov_pipe.unet = original_unet\n",
    "    return calibration_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a5d9c1dcbebb1a8",
   "metadata": {},
   "source": [
    "### Run Hybrid Model Quantization\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "586920796dacd8db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T14:31:36.656460Z",
     "start_time": "2024-02-13T13:32:03.144519Z"
    },
    "test_replace": {
     "calibration_dataset_size = 300": "calibration_dataset_size = 10",
     "num_inference_steps=50)": "num_inference_steps=10)"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031d12a102fd4953b6b78892179f6c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:nncf:Experimental Torch FX quantization backend is being used for the given torch.fx.GraphModule model. Torch FX PTQ is an experimental feature, consider using Torch or OpenVino PTQ backends in case of errors or a poor model performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Downloads/openvino_notebooks/.venv/lib/python3.10/site-packages/nncf/quantization/algorithms/post_training/pipeline.py:87: FutureWarning: `AdvancedQuantizationParameters(smooth_quant_alpha=..)` is deprecated.Please, use `AdvancedQuantizationParameters(smooth_quant_alphas)` option with AdvancedSmoothQuantParameters(convolution=.., matmul=..) as value instead.\n",
      "  warning_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:216 ignored nodes were found by names in the NNCFGraph\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 109 linear_9\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 111 linear_10\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1140 linear_106\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1142 linear_107\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1261 linear_119\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1263 linear_120\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1382 linear_132\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1384 linear_133\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1508 linear_145\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1510 linear_146\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1629 linear_158\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1631 linear_159\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1750 linear_171\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1752 linear_172\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1876 linear_184\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1878 linear_185\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1997 linear_197\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1999 linear_198\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2118 linear_210\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2120 linear_211\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 225 linear_22\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 227 linear_23\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 349 linear_35\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 351 linear_36\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 465 linear_48\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 467 linear_49\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 589 linear_61\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 591 linear_62\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 705 linear_74\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 707 linear_75\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 883 linear_89\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 885 linear_90\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 27 linear\n",
      "29 add__tensor_66\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 32 linear_1\n",
      "34 add__tensor_67\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1003 linear_97\n",
      "1005 add__tensor_121\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1037 linear_98\n",
      "1039 add__tensor_122\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1076 linear_99\n",
      "1078 add__tensor_123\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1197 linear_112\n",
      "1199 add__tensor_130\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1318 linear_125\n",
      "1320 add__tensor_137\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1444 linear_138\n",
      "1446 add__tensor_144\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1565 linear_151\n",
      "1567 add__tensor_151\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 165 linear_15\n",
      "167 add__tensor_75\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1686 linear_164\n",
      "1688 add__tensor_158\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1812 linear_177\n",
      "1814 add__tensor_165\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1933 linear_190\n",
      "1935 add__tensor_172\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2054 linear_203\n",
      "2056 add__tensor_179\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 285 linear_28\n",
      "287 add__tensor_82\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 405 linear_41\n",
      "407 add__tensor_89\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 49 linear_2\n",
      "51 add__tensor_68\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 525 linear_54\n",
      "527 add__tensor_96\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 645 linear_67\n",
      "647 add__tensor_103\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 765 linear_80\n",
      "767 add__tensor_110\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 794 linear_81\n",
      "796 add__tensor_111\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 823 linear_82\n",
      "825 add__tensor_112\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 939 linear_95\n",
      "941 add__tensor_119\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 969 linear_96\n",
      "971 add__tensor_120\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 74 linear_3\n",
      "76 add__tensor_69\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 81 linear_4\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 83 linear_5\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 85 linear_6\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 97 linear_7\n",
      "99 add__tensor_70\n",
      "101 div_2\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 107 linear_8\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 123 linear_11\n",
      "125 add__tensor_71\n",
      "127 div_3\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 133 linear_12\n",
      "135 add__tensor_72\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 143 linear_13\n",
      "145 add__tensor_73\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 148 linear_14\n",
      "150 add__tensor_74\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 190 linear_16\n",
      "192 add__tensor_76\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 197 linear_17\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 199 linear_18\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 201 linear_19\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 213 linear_20\n",
      "215 add__tensor_77\n",
      "217 div_5\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 223 linear_21\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 239 linear_24\n",
      "241 add__tensor_78\n",
      "243 div_6\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 249 linear_25\n",
      "251 add__tensor_79\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 259 linear_26\n",
      "261 add__tensor_80\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 264 linear_27\n",
      "266 add__tensor_81\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 314 linear_29\n",
      "316 add__tensor_83\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 321 linear_30\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 323 linear_31\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 325 linear_32\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 337 linear_33\n",
      "339 add__tensor_84\n",
      "341 div_8\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 347 linear_34\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 363 linear_37\n",
      "365 add__tensor_85\n",
      "367 div_9\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 373 linear_38\n",
      "375 add__tensor_86\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 383 linear_39\n",
      "385 add__tensor_87\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 388 linear_40\n",
      "390 add__tensor_88\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 430 linear_42\n",
      "432 add__tensor_90\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 437 linear_43\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 439 linear_44\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 441 linear_45\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 453 linear_46\n",
      "455 add__tensor_91\n",
      "457 div_11\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 463 linear_47\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 479 linear_50\n",
      "481 add__tensor_92\n",
      "483 div_12\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 489 linear_51\n",
      "491 add__tensor_93\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 499 linear_52\n",
      "501 add__tensor_94\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 504 linear_53\n",
      "506 add__tensor_95\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 554 linear_55\n",
      "556 add__tensor_97\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 561 linear_56\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 563 linear_57\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 565 linear_58\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 577 linear_59\n",
      "579 add__tensor_98\n",
      "581 div_14\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 587 linear_60\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 603 linear_63\n",
      "605 add__tensor_99\n",
      "607 div_15\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 613 linear_64\n",
      "615 add__tensor_100\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 623 linear_65\n",
      "625 add__tensor_101\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 628 linear_66\n",
      "630 add__tensor_102\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 670 linear_68\n",
      "672 add__tensor_104\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 677 linear_69\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 679 linear_70\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 681 linear_71\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 693 linear_72\n",
      "695 add__tensor_105\n",
      "697 div_17\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 703 linear_73\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 719 linear_76\n",
      "721 add__tensor_106\n",
      "723 div_18\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 729 linear_77\n",
      "731 add__tensor_107\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 739 linear_78\n",
      "741 add__tensor_108\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 744 linear_79\n",
      "746 add__tensor_109\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 848 linear_83\n",
      "850 add__tensor_113\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 855 linear_84\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 857 linear_85\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 859 linear_86\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 871 linear_87\n",
      "873 add__tensor_114\n",
      "875 div_22\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 881 linear_88\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 897 linear_91\n",
      "899 add__tensor_115\n",
      "901 div_23\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 907 linear_92\n",
      "909 add__tensor_116\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 917 linear_93\n",
      "919 add__tensor_117\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 922 linear_94\n",
      "924 add__tensor_118\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1105 linear_100\n",
      "1107 add__tensor_124\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1112 linear_101\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1114 linear_102\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1116 linear_103\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1128 linear_104\n",
      "1130 add__tensor_125\n",
      "1132 div_29\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1138 linear_105\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1154 linear_108\n",
      "1156 add__tensor_126\n",
      "1158 div_30\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1164 linear_109\n",
      "1166 add__tensor_127\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1174 linear_110\n",
      "1176 add__tensor_128\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1179 linear_111\n",
      "1181 add__tensor_129\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1226 linear_113\n",
      "1228 add__tensor_131\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1233 linear_114\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1235 linear_115\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1237 linear_116\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1249 linear_117\n",
      "1251 add__tensor_132\n",
      "1253 div_32\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1259 linear_118\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1275 linear_121\n",
      "1277 add__tensor_133\n",
      "1279 div_33\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1285 linear_122\n",
      "1287 add__tensor_134\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1295 linear_123\n",
      "1297 add__tensor_135\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1300 linear_124\n",
      "1302 add__tensor_136\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1347 linear_126\n",
      "1349 add__tensor_138\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1354 linear_127\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1356 linear_128\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1358 linear_129\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1370 linear_130\n",
      "1372 add__tensor_139\n",
      "1374 div_35\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1380 linear_131\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1396 linear_134\n",
      "1398 add__tensor_140\n",
      "1400 div_36\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1406 linear_135\n",
      "1408 add__tensor_141\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1416 linear_136\n",
      "1418 add__tensor_142\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1421 linear_137\n",
      "1423 add__tensor_143\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1473 linear_139\n",
      "1475 add__tensor_145\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1480 linear_140\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1482 linear_141\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1484 linear_142\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1496 linear_143\n",
      "1498 add__tensor_146\n",
      "1500 div_38\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1506 linear_144\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1522 linear_147\n",
      "1524 add__tensor_147\n",
      "1526 div_39\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1532 linear_148\n",
      "1534 add__tensor_148\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1542 linear_149\n",
      "1544 add__tensor_149\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1547 linear_150\n",
      "1549 add__tensor_150\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1594 linear_152\n",
      "1596 add__tensor_152\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1601 linear_153\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1603 linear_154\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1605 linear_155\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1617 linear_156\n",
      "1619 add__tensor_153\n",
      "1621 div_41\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1627 linear_157\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1643 linear_160\n",
      "1645 add__tensor_154\n",
      "1647 div_42\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1653 linear_161\n",
      "1655 add__tensor_155\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1663 linear_162\n",
      "1665 add__tensor_156\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1668 linear_163\n",
      "1670 add__tensor_157\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1715 linear_165\n",
      "1717 add__tensor_159\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1722 linear_166\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1724 linear_167\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1726 linear_168\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1738 linear_169\n",
      "1740 add__tensor_160\n",
      "1742 div_44\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1748 linear_170\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1764 linear_173\n",
      "1766 add__tensor_161\n",
      "1768 div_45\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1774 linear_174\n",
      "1776 add__tensor_162\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1784 linear_175\n",
      "1786 add__tensor_163\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1789 linear_176\n",
      "1791 add__tensor_164\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1841 linear_178\n",
      "1843 add__tensor_166\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1848 linear_179\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1850 linear_180\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1852 linear_181\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1864 linear_182\n",
      "1866 add__tensor_167\n",
      "1868 div_47\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1874 linear_183\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1890 linear_186\n",
      "1892 add__tensor_168\n",
      "1894 div_48\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1900 linear_187\n",
      "1902 add__tensor_169\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1910 linear_188\n",
      "1912 add__tensor_170\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1915 linear_189\n",
      "1917 add__tensor_171\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1962 linear_191\n",
      "1964 add__tensor_173\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1969 linear_192\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1971 linear_193\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1973 linear_194\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1985 linear_195\n",
      "1987 add__tensor_174\n",
      "1989 div_50\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 1995 linear_196\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2011 linear_199\n",
      "2013 add__tensor_175\n",
      "2015 div_51\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2021 linear_200\n",
      "2023 add__tensor_176\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2031 linear_201\n",
      "2033 add__tensor_177\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2036 linear_202\n",
      "2038 add__tensor_178\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2083 linear_204\n",
      "2085 add__tensor_180\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2090 linear_205\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2092 linear_206\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2094 linear_207\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2106 linear_208\n",
      "2108 add__tensor_181\n",
      "2110 div_53\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2116 linear_209\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2132 linear_212\n",
      "2134 add__tensor_182\n",
      "2136 div_54\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2142 linear_213\n",
      "2144 add__tensor_183\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2152 linear_214\n",
      "2154 add__tensor_184\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2157 linear_215\n",
      "2159 add__tensor_185\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d4de9895a84e10ae150bbb0f0f596d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef6b3712d664a7db5cbb2991f44bb30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%skip not $to_quantize.value\n",
    "from collections import deque\n",
    "from transformers import set_seed\n",
    "import torch \n",
    "import nncf\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "def collect_ops_with_weights(graph_module):\n",
    "    ops_with_weights = []\n",
    "    for node in graph_module.graph.nodes:\n",
    "        if \"linear\" in node.name:\n",
    "            ops_with_weights.append(node.name)\n",
    "    return ops_with_weights\n",
    "\n",
    "\n",
    "calibration_dataset_size = 300\n",
    "set_seed(1)\n",
    "unet_calibration_data = collect_calibration_data(ov_pipe,\n",
    "                                                    calibration_dataset_size=calibration_dataset_size,\n",
    "                                                    num_inference_steps=1)\n",
    "\n",
    "unet_ignored_scope = collect_ops_with_weights(ov_pipe.unet)\n",
    "with disable_patching():\n",
    "    with torch.no_grad():\n",
    "        quantized_unet = nncf.quantize(\n",
    "            model=ov_pipe.unet,\n",
    "            calibration_dataset=nncf.Dataset(unet_calibration_data),\n",
    "            subset_size=calibration_dataset_size,\n",
    "            model_type=nncf.ModelType.TRANSFORMER,\n",
    "            ignored_scope=nncf.IgnoredScope(names=unet_ignored_scope),\n",
    "            advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alpha=-1)\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5d398a5a7b87506",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T14:32:27.192039Z",
     "start_time": "2024-02-13T14:32:23.567293Z"
    }
   },
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "int8_ov_pipe = OVStableDiffusionPipeline(\n",
    "    tokenizer=tokenizer,\n",
    "    text_encoder=text_enc,\n",
    "    unet=quantized_unet,\n",
    "    vae_encoder=vae_encoder,\n",
    "    vae_decoder=vae_decoder,\n",
    "    scheduler=scheduler\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "637babca1053f709",
   "metadata": {},
   "source": [
    "### Compare inference time of the FP16 and INT8 pipelines\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "To measure the inference performance of the `FP16` and `INT8` pipelines, we use median inference time on calibration subset.\n",
    "\n",
    "> **NOTE**: For the most accurate performance estimation, it is recommended to run `benchmark_app` in a terminal/command prompt after closing other applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b653073baccacaa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T14:32:29.452397Z",
     "start_time": "2024-02-13T14:32:29.449204Z"
    }
   },
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "import time\n",
    "import openvino.torch\n",
    "from nncf.experimental.torch.fx.nncf_graph_builder import GraphConverter\n",
    "\n",
    "def calculate_inference_time(model, dataset):\n",
    "    model_compiled = torch.compile(model, backend='openvino')\n",
    "    with torch.no_grad():\n",
    "        _ = model_compiled(*dataset[0])\n",
    "        inference_time = []\n",
    "        pbar = tqdm(total=10)\n",
    "        for data in dataset:\n",
    "            start = time.perf_counter()\n",
    "            model_compiled(*data)\n",
    "            end = time.perf_counter()\n",
    "            delta = end - start\n",
    "            inference_time.append(delta)\n",
    "            pbar.update(len(inference_time) - pbar.n)\n",
    "    return np.median(inference_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43478ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc28359552fd4419be0fd0c256f5e478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cac093107504d72a70624e276b11b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of FP: 4.122\n",
      "Performance of INT8: 3.252\n"
     ]
    }
   ],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "validation_data = unet_calibration_data[:10]\n",
    "fp_latency = calculate_inference_time(ov_pipe.unet, validation_data)\n",
    "int8_latency = calculate_inference_time(int8_ov_pipe.unet, validation_data)\n",
    "print(f\"Performance of FP: {fp_latency:.3f}\")\n",
    "print(f\"Performance of INT8: {int8_latency:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b7f93ec1bd55a2f",
   "metadata": {},
   "source": [
    "## Run Text-to-Image generation\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Now, you can define a text prompts for image generation and run inference pipeline.\n",
    "Optionally, you can also change the random generator seed for latent state initialization and number of steps.\n",
    "\n",
    "> **Note**: Consider increasing `steps` to get more precise results. A suggested value is `50`, but it will take longer time to process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd89fdab77ca1f8c",
   "metadata": {},
   "source": [
    "Please select below whether you would like to use the quantized model to launch the interactive demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "edd734469b49f2f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T14:44:53.376186Z",
     "start_time": "2024-02-13T14:44:53.371802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36946544510436c9998634296aa9c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Use quantized model')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model_present = int8_ov_pipe is not None\n",
    "\n",
    "use_quantized_model = widgets.Checkbox(\n",
    "    value=True if quantized_model_present else False,\n",
    "    description=\"Use quantized model\",\n",
    "    disabled=not quantized_model_present,\n",
    ")\n",
    "\n",
    "use_quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1136e88de27fec2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T14:44:55.067276Z",
     "start_time": "2024-02-13T14:44:53.376929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
      "----\n",
      "Running on public URL: https://edab5392347df7a9de.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://edab5392347df7a9de.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "pipeline = int8_ov_pipe if use_quantized_model.value else ov_pipe\n",
    "\n",
    "\n",
    "def generate(prompt, negative_prompt, seed, num_steps, _=gr.Progress(track_tqdm=True)):\n",
    "    result = pipeline(\n",
    "        prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_inference_steps=num_steps,\n",
    "        seed=seed,\n",
    "    )\n",
    "    return result[\"sample\"][0]\n",
    "\n",
    "\n",
    "gr.close_all()\n",
    "demo = gr.Interface(\n",
    "    generate,\n",
    "    [\n",
    "        gr.Textbox(\n",
    "            \"valley in the Alps at sunset, epic vista, beautiful landscape, 4k, 8k\",\n",
    "            label=\"Prompt\",\n",
    "        ),\n",
    "        gr.Textbox(\n",
    "            \"frames, borderline, text, charachter, duplicate, error, out of frame, watermark, low quality, ugly, deformed, blur\",\n",
    "            label=\"Negative prompt\",\n",
    "        ),\n",
    "        gr.Slider(value=42, label=\"Seed\", maximum=10000000),\n",
    "        gr.Slider(value=25, label=\"Steps\", minimum=1, maximum=50),\n",
    "    ],\n",
    "    \"image\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    demo.queue().launch()\n",
    "except Exception:\n",
    "    demo.queue().launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/stable-diffusion-v2/stable-diffusion-v2-optimum-demo.png?raw=true",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [
     "Stable Diffusion"
    ],
    "tasks": [
     "Text-to-Image"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
