{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion v2 Demo with Torch Compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"diffusers>=0.14.0\" openvino-nightly \"datasets>=2.14.6\" \"transformers>=4.25.1\" \"gradio>=4.19\" \"torch>=2.1\" Pillow opencv-python --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q git+https://github.com/anzr299/nncf.git\n",
    "%pip install -q accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Diffusion v2 for Text-to-Image Generation\n",
    "\n",
    "To start, let's look on Text-to-Image process for Stable Diffusion v2. We will use [Stable Diffusion v2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1) model for these purposes. The main difference from Stable Diffusion v2 and Stable Diffusion v2.1 is usage of more data, more training, and less restrictive filtering of the dataset, that gives promising results for selecting wide range of input text prompts. More details about model can be found in [Stability AI blog post](https://stability.ai/blog/stablediffusion2-1-release7-dec-2022) and original model [repository](https://github.com/Stability-AI/stablediffusion).\n",
    "\n",
    "### Stable Diffusion in Diffusers library\n",
    "To work with Stable Diffusion v2, we will use Hugging Face [Diffusers](https://github.com/huggingface/diffusers) library. To experiment with Stable Diffusion models, Diffusers exposes the [`StableDiffusionPipeline`](https://huggingface.co/docs/diffusers/using-diffusers/conditional_image_generation) similar to the [other Diffusers pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview).  The code below demonstrates how to create `StableDiffusionPipeline` using `stable-diffusion-2-1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n",
      "WARNING:nncf:NNCF provides best results with torch==2.4.*, while current torch version is 2.3.1+cpu. If you encounter issues, consider switching to torch==2.4.*\n"
     ]
    }
   ],
   "source": [
    "from torch._export import capture_pre_autograd_graph\n",
    "from nncf.torch.dynamic_graph.patch_pytorch import disable_patching\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4211a5b2f6734bc999b1bb724509099d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/706 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926c5a5d1c4c41d49308e43cc95e2807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8aa0192b89540a8829067fa5bcc518c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder_2/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b7188a0b214e2f9602fe4d704b987c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler/scheduler_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214e27f3cff4494994b4c21432b0a498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder_3/config.json:   0%|          | 0.00/740 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07519a96ada9474cad5815757b3fe6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/config.json:   0%|          | 0.00/574 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad3b77c88db409b8905e35ac61f4901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)t_encoder_3/model.safetensors.index.json:   0%|          | 0.00/19.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfede6fc4fb4fb584c0c404b29f0a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cb4f54472f43f5baedfed1fa40d146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/special_tokens_map.json:   0%|          | 0.00/588 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8975ff84a67f4111bf68c07960f4480f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/tokenizer_config.json:   0%|          | 0.00/705 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226722a863ac4542812ccab281cf05bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.39G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac2dbf591c74e4facd8aceeef89a5f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f39ba8e623497fae93ee107e4d49b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce08c2882a6454faaa4a6cbd8d55f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/247M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474daeba259c4d4cb1e7e49dee5e680c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d480e40406c42fdbafde93c6dbfbdf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_2/special_tokens_map.json:   0%|          | 0.00/576 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3892eaa46444c2c8a6b5aed381979a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_2/tokenizer_config.json:   0%|          | 0.00/856 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0bb965ce064e59be5a6fb86c23dce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_3/special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b0358a76bf46ed8a0938e546830498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_3/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaaf4f96d87247c299eea67110435f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_3/tokenizer_config.json:   0%|          | 0.00/20.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ac236354c3433fbd658fc220b96cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transformer/config.json:   0%|          | 0.00/372 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d27edbbd3114cfeaa94de5afd29d763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/config.json:   0%|          | 0.00/739 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478c5d4c31eb4b56be2712722f78b140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42385cdd16024832b1de27d13b2da06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/4.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c02c7fc1954432a52dd8cb6c960893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059c73821dd64780a668ddec76ef0c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b602239ca684979a591d456a870bf8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import StableDiffusion3Pipeline\n",
    "import torch\n",
    "import random\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(42)\n",
    "pipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\")\n",
    "pipe.to(\"cpu\")\n",
    "\n",
    "# if using torch < 2.0\n",
    "# pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "prompt = \"valley in the Alps at sunset, epic vista, beautiful landscape, 4k, 8k\"\n",
    "negative_prompt = \"frames, borderline, text, charachter, duplicate, error, out of frame, watermark, low quality, ugly, deformed, blur\"\n",
    "\n",
    "# images = pipe(prompt=prompt, negative_prompt=negative_prompt, num_inference_steps=25, generator=generator).images[0]\n",
    "# images.show()\n",
    "# images.save(\"/home/user/Downloads/stable-diffusion-xl-experiments/stable_diffusion_xl_all_pytorch_model.png\")# del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.text_encoder = torch.compile(pipe.text_encoder, backend='openvino')\n",
    "pipe.text_encoder_2 = torch.compile(pipe.text_encoder_2, backend='openvino')\n",
    "pipe.vae.decoder = torch.compile(pipe.vae.decoder, backend='openvino')\n",
    "pipe.vae.encoder = torch.compile(pipe.vae.encoder, backend='openvino')\n",
    "pipe.unet = torch.compile(pipe.unet, backend='openvino')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    model_size_mb = (param_size + buffer_size) / 1024**2\n",
    "\n",
    "    return model_size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9794.096694946289"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_size(pipe.unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41bf00adcab942a99218dade5da92969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "Reached\n",
      "torch.Size([1, 4, 128, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b190adeeea4a9db3b1a0508b529494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "torch.Size([2, 1280]) torch.Size([2, 6]) torch.Size([2, 4, 128, 128]) torch.Size([]) encoder_hidden_states= torch.Size([2, 77, 2048]) cross_attention_kwarg None\n",
      "Reached\n",
      "torch.Size([1, 4, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "with torch.no_grad():\n",
    "    pipe(prompt ,num_inference_steps=1)\n",
    "    image = pipe(prompt=prompt, negative_prompt=negative_prompt ,num_inference_steps=25, generator=generator).images[0]\n",
    "image.show()\n",
    "image.save(\"/home/user/Downloads/stable-diffusion-xl-experiments/stable_diffusion_xl_all_torch_compile_model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Models to Torch Fx Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_input = torch.ones((1, 77), dtype=torch.long)\n",
    "text_encoder_2_input = torch.ones((1, 77), dtype=torch.long)\n",
    "vae_encoder_input = torch.ones((1, 3, 256, 256))\n",
    "vae_decoder_input = torch.ones((1, 4, 128, 128))\n",
    "vae_decoder_kwargs = {}\n",
    "vae_decoder_kwargs[\"return_dict\"] = False\n",
    "latents_shape = (2, 4, 128, 128)\n",
    "latents = torch.randn(latents_shape)\n",
    "t = torch.from_numpy(np.array(1, dtype=np.float32))\n",
    "added_cond_kwargs = {}\n",
    "added_cond_kwargs[\"text_embeds\"] = torch.ones((2, 1280))\n",
    "added_cond_kwargs[\"time_ids\"] = torch.ones((2,6))\n",
    "unet_kwargs = {}\n",
    "unet_kwargs[\"encoder_hidden_states\"] = torch.ones((2, 77, 2048))\n",
    "unet_kwargs[\"added_cond_kwargs\"] = added_cond_kwargs\n",
    "unet_kwargs[\"return_dict\"] = False\n",
    "unet_input = (latents, t)\n",
    "\n",
    "text_encoder_kwargs = {}\n",
    "text_encoder_kwargs['output_hidden_states'] = True\n",
    "\n",
    "with torch.no_grad():\n",
    "    with disable_patching():\n",
    "        text_encoder = capture_pre_autograd_graph(pipe.text_encoder.eval(), args=(text_encoder_input,), kwargs=(text_encoder_kwargs))\n",
    "        text_encoder_2 = capture_pre_autograd_graph(pipe.text_encoder_2.eval(), args=(text_encoder_2_input,), kwargs=(text_encoder_kwargs))\n",
    "        vae_encoder = capture_pre_autograd_graph(pipe.vae.encoder, args=(vae_encoder_input,))\n",
    "        vae_decoder = capture_pre_autograd_graph(pipe.vae.decoder.eval(), args=(vae_decoder_input,))\n",
    "        unet = capture_pre_autograd_graph(pipe.unet.eval(), args=(*unet_input,), kwargs=(unet_kwargs))\n",
    "del added_cond_kwargs\n",
    "del unet_kwargs\n",
    "del unet_input\n",
    "del latents\n",
    "del t\n",
    "del vae_encoder_input\n",
    "del vae_decoder_input\n",
    "del text_encoder_2_input\n",
    "del text_encoder_input\n",
    "del text_encoder_kwargs\n",
    "del vae_decoder_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.text_encoder = text_encoder\n",
    "pipe.text_encoder_2 = text_encoder_2\n",
    "pipe.vae.decoder = vae_decoder\n",
    "pipe.vae.encoder = vae_encoder\n",
    "pipe.unet = unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Any, Dict, List\n",
    "import torch\n",
    "\n",
    "def disable_progress_bar(pipeline, disable=True):\n",
    "    if not hasattr(pipeline, \"_progress_bar_config\"):\n",
    "        pipeline._progress_bar_config = {'disable': disable}\n",
    "    else:\n",
    "        pipeline._progress_bar_config['disable'] = disable\n",
    "\n",
    "\n",
    "class UNetWrapper(torch.nn.Module):\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.captured_args = []\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        if np.random.rand() <= 0.7:\n",
    "            self.captured_args.append((*args, *tuple(kwargs.values())))\n",
    "        return self.unet(*args, **kwargs)\n",
    "\n",
    "def collect_calibration_data(ov_pipe, calibration_dataset_size: int, num_inference_steps: int) -> List[Dict]:\n",
    "    \n",
    "    original_unet = ov_pipe.unet\n",
    "    calibration_data = []\n",
    "    disable_progress_bar(ov_pipe)\n",
    "\n",
    "    dataset = datasets.load_dataset(\"google-research-datasets/conceptual_captions\", split=\"train\", trust_remote_code=True).shuffle(seed=42)\n",
    "\n",
    "    wrapped_unet = UNetWrapper(ov_pipe.unet)\n",
    "    ov_pipe.unet = wrapped_unet\n",
    "    print(ov_pipe.unet)\n",
    "    # Run inference for data collection\n",
    "    pbar = tqdm(total=calibration_dataset_size)\n",
    "    for i, batch in enumerate(dataset):\n",
    "        prompt = batch[\"caption\"]\n",
    "        print(prompt)\n",
    "        if len(prompt) > ov_pipe.tokenizer.model_max_length:\n",
    "            continue\n",
    "        # Run the pipeline\n",
    "        ov_pipe(prompt, num_inference_steps=num_inference_steps, seed=1)\n",
    "        print(\"pipe completed\")\n",
    "        calibration_data.extend(wrapped_unet.captured_args)\n",
    "        wrapped_unet.captured_args = []\n",
    "        pbar.update(len(calibration_data) - pbar.n)\n",
    "        if pbar.n >= calibration_dataset_size:\n",
    "            break\n",
    "\n",
    "    disable_progress_bar(ov_pipe, disable=False)\n",
    "    ov_pipe.unet = original_unet\n",
    "    return calibration_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"test_faster\", \"wb\") as fp:\n",
    "    pickle.dump(unet_calibration_data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_faster\", \"rb\") as fp:\n",
    "    unet_calibration_data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_ops_with_weights(graph_module):\n",
    "    ops_with_weights = []\n",
    "    for node in graph_module.graph.nodes:\n",
    "        if \"linear\" in node.name:\n",
    "            ops_with_weights.append(node.name)\n",
    "    return ops_with_weights\n",
    "\n",
    "\n",
    "calibration_dataset_size = 30\n",
    "unet_calibration_data = collect_calibration_data(pipe,\n",
    "                                                    calibration_dataset_size=calibration_dataset_size,\n",
    "                                                    num_inference_steps=10)\n",
    "unet_ignored_scope = collect_ops_with_weights(pipe.unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nncf\n",
    "from nncf.quantization.advanced_parameters import AdvancedSmoothQuantParameters\n",
    "from nncf.quantization.range_estimator import RangeEstimatorParametersSet\n",
    "\n",
    "# compressed_text_encoder = nncf.compress_weights(text_encoder)\n",
    "# compressed_text_encoder_2 = nncf.compress_weights(text_encoder_2)\n",
    "# compressed_vae_encoder = nncf.compress_weights(vae_encoder)\n",
    "# compressed_vae_decoder = nncf.compress_weights(vae_decoder)\n",
    "with disable_patching():\n",
    "    with torch.no_grad():\n",
    "        # compressed_unet = nncf.compress_weights(unet, ignored_scope=nncf.IgnoredScope(types=['conv2d']))\n",
    "        quantized_unet = nncf.quantize( #1\n",
    "            model=pipe.unet,\n",
    "            calibration_dataset=nncf.Dataset(unet_calibration_data),\n",
    "            subset_size=len(unet_calibration_data),\n",
    "            model_type=nncf.ModelType.TRANSFORMER,\n",
    "            ignored_scope=nncf.IgnoredScope(names=unet_ignored_scope),\n",
    "            advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alpha=-1, disable_bias_correction=True)\n",
    "        )\n",
    "        # quantized_unet = nncf.quantize( #2\n",
    "        #     model=pipe.unet,\n",
    "        #     calibration_dataset=nncf.Dataset(unet_calibration_data),\n",
    "        #     subset_size=len(unet_calibration_data),\n",
    "        #     model_type=nncf.ModelType.TRANSFORMER,\n",
    "        #     fast_bias_correction=False,\n",
    "        #     ignored_scope=nncf.IgnoredScope(names=unet_ignored_scope),\n",
    "        #     advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alpha=-1, disable_bias_correction=True, weights_range_estimator_params=RangeEstimatorParametersSet.MINMAX, activations_range_estimator_params=RangeEstimatorParametersSet.MINMAX)\n",
    "        # )\n",
    "        # quantized_unet = nncf.quantize( #3\n",
    "        #     model=pipe.unet,\n",
    "        #     calibration_dataset=nncf.Dataset(unet_calibration_data),\n",
    "        #     subset_size=len(unet_calibration_data),\n",
    "        #     model_type=nncf.ModelType.TRANSFORMER,\n",
    "        #     fast_bias_correction=False,\n",
    "        #     ignored_scope=nncf.IgnoredScope(names=unet_ignored_scope),\n",
    "        #     advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alpha=-1)\n",
    "        # )\n",
    "        # quantized_unet = nncf.quantize( #4.1\n",
    "        #     model=pipe.unet,\n",
    "        #     calibration_dataset=nncf.Dataset(unet_calibration_data),\n",
    "        #     subset_size=len(unet_calibration_data),\n",
    "        #     model_type=nncf.ModelType.TRANSFORMER,\n",
    "        #     fast_bias_correction=False,\n",
    "        #     ignored_scope=nncf.IgnoredScope(names=unet_ignored_scope),\n",
    "        #     advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alphas=AdvancedSmoothQuantParameters(convolution=0.95, matmul=-1))\n",
    "        # )\n",
    "        # quantized_unet = nncf.quantize( #4.2\n",
    "        #     model=pipe.unet,\n",
    "        #     calibration_dataset=nncf.Dataset(unet_calibration_data),\n",
    "        #     subset_size=len(unet_calibration_data),\n",
    "        #     model_type=nncf.ModelType.TRANSFORMER,\n",
    "        #     fast_bias_correction=False,\n",
    "        #     ignored_scope=nncf.IgnoredScope(names=unet_ignored_scope),\n",
    "        #     advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alphas=AdvancedSmoothQuantParameters(convolution=0.95))\n",
    "        # )\n",
    "# del text_encoder\n",
    "# del text_encoder_2\n",
    "# del vae_decoder\n",
    "del unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Models with OV Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiled_compressed_text_encoder = torch.compile(compressed_text_encoder, backend='openvino')\n",
    "# compiled_compressed_text_encoder_2 = torch.compile(compressed_text_encoder_2, backend='openvino')\n",
    "compiled_compressed_unet = torch.compile(quantized_unet, backend='openvino')\n",
    "# compiled_compressed_vae_encoder = torch.compile(compressed_vae_encoder, backend='openvino')\n",
    "# compiled_compressed_vae_decoder = torch.compile(compressed_vae_decoder, backend='openvino')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del compressed_text_encoder\n",
    "del compressed_text_encoder_2\n",
    "del compressed_vae_decoder\n",
    "del compressed_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.text_encoder = compiled_compressed_text_encoder\n",
    "# pipe.text_encoder_2 = compiled_compressed_text_encoder_2\n",
    "# pipe.vae.encoder = compiled_compressed_vae_encoder\n",
    "# pipe.vae.decoder = compiled_compressed_vae_decoder\n",
    "pipe.unet = compiled_compressed_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.text_encoder = compressed_text_encoder\n",
    "pipe.text_encoder_2 = compressed_text_encoder_2\n",
    "pipe.vae.decoder = compressed_vae_decoder\n",
    "pipe.unet = compressed_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del compiled_compressed_text_encoder\n",
    "del compiled_compressed_text_encoder_2\n",
    "del compiled_compressed_vae_decoder\n",
    "del compiled_compressed_vae_encoder\n",
    "del compiled_compressed_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.text_encoder = text_encoder\n",
    "pipe.text_encoder_2 = text_encoder_2\n",
    "pipe.vae.decoder = vae_decoder\n",
    "# pipe.vae.encoder = vae_encoder\n",
    "# pipe.unet = unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warmup the model for initial compile\n",
    "prompt = \"valley in the Alps at sunset, epic vista, beautiful landscape, 4k, 8k\"\n",
    "negative_prompt = \"frames, borderline, text, charachter, duplicate, error, out of frame, watermark, low quality, ugly, deformed, blur\"\n",
    "num_steps = 1\n",
    "with torch.no_grad():\n",
    "    image = pipe(prompt=prompt, negative_prompt=negative_prompt, num_inference_steps=num_steps, generator=generator).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "with torch.no_grad():\n",
    "    image = pipe(prompt=prompt, negative_prompt=negative_prompt, num_inference_steps=25, generator=generator).images[0]\n",
    "image.show()\n",
    "image.save(\"stable_diffusion_xl_all_fx_unet_quantized_more_calibration.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refiner = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "    text_encoder_2=pipe.text_encoder_2,\n",
    "    vae=pipe.vae\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "with torch.no_grad():\n",
    "    image = pipe(prompt=prompt, negative_prompt=negative_prompt, num_inference_steps=25, generator=generator, output_type=\"latent\").images\n",
    "    image = refiner(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=25,\n",
    "    denoising_start=0.8,\n",
    "    image=image,\n",
    ").images[0]\n",
    "image.show()\n",
    "image.save(\"stable_diffusion_xl_all_fx_unet_quantized_more_calibration.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Inference\n",
    "Generating an image with the same parameters as the original OV Stable diffusion model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 25\n",
    "\n",
    "with torch.no_grad():\n",
    "    image = pipe(prompt, num_inference_steps=num_steps).images[0]\n",
    "image.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
