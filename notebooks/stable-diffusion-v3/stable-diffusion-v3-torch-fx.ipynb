{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image generation with Stable Diffusion v3 and OpenVINO\n",
    "\n",
    "Stable Diffusion V3 is next generation of latent diffusion image Stable Diffusion models family that  outperforms state-of-the-art text-to-image generation systems in typography and prompt adherence, based on human preference evaluations. In comparison with previous versions, it based on Multimodal Diffusion Transformer (MMDiT) text-to-image model that features greatly improved performance in image quality, typography, complex prompt understanding, and resource-efficiency.\n",
    "\n",
    "![mmdit.png](https://github.com/openvinotoolkit/openvino_notebooks/assets/29454499/dd079427-89f2-4d28-a10e-c80792d750bf)\n",
    "\n",
    "More details about model can be found in [model card](https://huggingface.co/stabilityai/stable-diffusion-3-medium), [research paper](https://stability.ai/news/stable-diffusion-3-research-paper) and [Stability.AI blog post](https://stability.ai/news/stable-diffusion-3-medium).\n",
    "In this tutorial, we will consider how to convert Stable Diffusion v3 for running with OpenVINO. An additional part demonstrates how to run optimization with [NNCF](https://github.com/openvinotoolkit/nncf/) to speed up pipeline.\n",
    "If you want to run previous Stable Diffusion versions, please check our other notebooks:\n",
    "\n",
    "* [Stable Diffusion](../stable-diffusion-text-to-image)\n",
    "* [Stable Diffusion v2](../stable-diffusion-v2)\n",
    "* [Stable Diffusion v3](../stable-diffusion-v3)\n",
    "* [Stable Diffusion XL](../stable-diffusion-xl)\n",
    "* [LCM Stable Diffusion](../latent-consistency-models-image-generation)\n",
    "* [Turbo SDXL](../sdxl-turbo)\n",
    "* [Turbo SD](../sketch-to-image-pix2pix-turbo)\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Build PyTorch pipeline](#Build-PyTorch-pipeline)\n",
    "- [Convert models with OpenVINO](#Convert-models-with-OpenVINO)\n",
    "    - [Transformer](#Transformer)\n",
    "    - [T5 Text Encoder](#T5-Text-Encoder)\n",
    "    - [Clip text encoders](#Clip-text-encoders)\n",
    "    - [VAE](#VAE)\n",
    "- [Prepare OpenVINO inference pipeline](#Prepare-OpenVINO-inference-pipeline)\n",
    "- [Run OpenVINO model](#Run-OpenVINO-model)\n",
    "- [Quantization](#Quantization)\n",
    "    - [Prepare calibration dataset](#Prepare-calibration-dataset)\n",
    "    - [Run Quantization](#Run-Quantization)\n",
    "    - [Run Weights Compression](#Run-Weights-Compression)\n",
    "    - [Compare model file sizes](#Compare-model-file-sizes)\n",
    "    - [Compare inference time of the FP16 and optimized pipelines](#Compare-inference-time-of-the-FP16-and-optimized-pipelines)\n",
    "- [Interactive demo](#Interactive-demo)\n",
    "\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide).\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/stable-diffusion-v3/stable-diffusion-v3.ipynb\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"diffusers>=0.14.0\" \"gradio>=4.19\" \"torch>=2.1\" \"transformers\" \"nncf>=2.12.0\" \"datasets>=2.14.6\" \"opencv-python\" \"pillow\" \"peft>=0.7.0\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -qU \"openvino>=2024.3.0\"\n",
    "%pip install git+https://github.com/openvinotoolkit/nncf.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build PyTorch pipeline\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    ">**Note**: run model with notebook, you will need to accept license agreement. \n",
    ">You must be a registered user in ü§ó Hugging Face Hub. Please visit [HuggingFace model card](https://huggingface.co/stabilityai/stable-diffusion-3-medium-diffusers), carefully read terms of usage and click accept button.  You will need to use an access token for the code below to run. For more information on access tokens, refer to [this section of the documentation](https://huggingface.co/docs/hub/security-tokens).\n",
    ">You can login on Hugging Face Hub in notebook environment, using following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment these lines to login to huggingfacehub to get access to pretrained model\n",
    "\n",
    "# from huggingface_hub import notebook_login, whoami\n",
    "\n",
    "# try:\n",
    "#     whoami()\n",
    "#     print('Authorization token already provided')\n",
    "# except OSError:\n",
    "#     notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusion3Pipeline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "pipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\", text_encoder_3=None, tokenizer_3=None)\n",
    "pipe.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the Configs\n",
    "\n",
    "This will be used later when wrapping the Torch FX models to insert back into the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_config = pipe.text_encoder.config\n",
    "text_encoder_2_config = pipe.text_encoder_2.config\n",
    "transformer_config = pipe.transformer.config\n",
    "vae_config = pipe.vae.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run FP Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "latents = np.random.randn(1, 16, 128, 128).astype(np.float32)\n",
    "latents = torch.from_numpy(latents).to(\"cpu\")\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(42)\n",
    "prompt = \"valley in the Alps at sunset, epic vista, beautiful landscape, 4k, 8k\"\n",
    "# prompt = 'A raccoon trapped inside a glass jar full of colorful candies, the background is steamy with vivid colors'\n",
    "with torch.no_grad():\n",
    "    image = pipe(prompt=prompt, negative_prompt='', num_inference_steps=1, generator=generator, guidance_scale=5).images[0]\n",
    "image.resize((512, 512,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert models to Torch FX\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "This step converts the pytorch models in the hf pipeline to Torch FX representation using the `capture_pre_autograd()` function. \n",
    "\n",
    "\n",
    "The pipeline consists of four important parts:\n",
    "\n",
    "* Clip and T5 Text Encoders to create condition to generate an image from a text prompt.\n",
    "* Transformer for step-by-step denoising latent image representation.\n",
    "* Autoencoder (VAE) for decoding latent space to image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch._export import capture_pre_autograd_graph\n",
    "from nncf.torch.dynamic_graph.patch_pytorch import disable_patching\n",
    "\n",
    "text_encoder_input = torch.ones((1, 77), dtype=torch.long)\n",
    "text_encoder_kwargs = {}\n",
    "text_encoder_kwargs['output_hidden_states'] = True\n",
    "\n",
    "vae_encoder_input = torch.ones((1, 3, 128, 128))\n",
    "vae_decoder_input = torch.ones((1, 16, 128, 128))\n",
    "\n",
    "unet_kwargs = {}\n",
    "unet_kwargs[\"hidden_states\"] = torch.ones((2, 16, 128, 128))\n",
    "unet_kwargs[\"timestep\"] = torch.from_numpy(np.array([1,2], dtype=np.float32))\n",
    "unet_kwargs[\"encoder_hidden_states\"] = torch.ones((2, 154, 4096))\n",
    "unet_kwargs[\"pooled_projections\"] = torch.ones((2, 2048))\n",
    "unet_kwargs[\"joint_attention_kwargs\"] = None\n",
    "unet_kwargs[\"return_dict\"] = False\n",
    "\n",
    "with torch.no_grad():\n",
    "    with disable_patching():\n",
    "        pipe.text_encoder = capture_pre_autograd_graph(pipe.text_encoder.eval(), args=(text_encoder_input,), kwargs=(text_encoder_kwargs))\n",
    "        pipe.text_encoder_2 = capture_pre_autograd_graph(pipe.text_encoder_2.eval(), args=(text_encoder_input,), kwargs=(text_encoder_kwargs))\n",
    "        pipe.vae.decoder = capture_pre_autograd_graph(pipe.vae.decoder, args=(vae_decoder_input,))\n",
    "        pipe.vae.encoder = capture_pre_autograd_graph(pipe.vae.encoder, args=(vae_encoder_input,))\n",
    "        pipe.transformer = capture_pre_autograd_graph(pipe.transformer.eval(), args=(), kwargs=(unet_kwargs))\n",
    "del unet_kwargs\n",
    "del vae_encoder_input\n",
    "del vae_decoder_input\n",
    "del text_encoder_input\n",
    "del text_encoder_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "[NNCF](https://github.com/openvinotoolkit/nncf/) enables post-training quantization by adding quantization layers into model graph and then using a subset of the training dataset to initialize the parameters of these additional quantization layers. Quantized operations are executed in `INT8` instead of `FP32`/`FP16` making model inference faster.\n",
    "\n",
    "According to `StableDiffusion3Pipeline` structure, the `transformer` model takes up significant portion of the overall pipeline execution time. Now we will show you how to optimize the transformer part using [NNCF](https://github.com/openvinotoolkit/nncf/) to reduce computation cost and speed up the pipeline. Quantizing the rest of the pipeline does not significantly improve inference performance but can lead to a substantial degradation of accuracy. That's why we use 4-bit weight compression for the rest of the pipeline to reduce memory footprint.\n",
    "\n",
    "Please select below whether you would like to run quantization to improve model inference speed.\n",
    "\n",
    "> **NOTE**: Quantization is time and memory consuming operation. Running quantization code below may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b33a7ae86324bf1aa21bd5578ac4128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Quantization')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from notebook_utils import quantization_widget\n",
    "from sd3_quantization_helper import TRANSFORMER_INT8_PATH, TEXT_ENCODER_INT4_PATH, TEXT_ENCODER_2_INT4_PATH, TEXT_ENCODER_3_INT4_PATH, VAE_DECODER_INT4_PATH\n",
    "\n",
    "to_quantize = quantization_widget()\n",
    "\n",
    "to_quantize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Calibration Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "import datasets\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "def disable_progress_bar(pipeline, disable=True):\n",
    "    if not hasattr(pipeline, \"_progress_bar_config\"):\n",
    "        pipeline._progress_bar_config = {'disable': disable}\n",
    "    else:\n",
    "        pipeline._progress_bar_config['disable'] = disable\n",
    "\n",
    "\n",
    "class UNetWrapper(torch.nn.Module):\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.captured_args = []\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        if np.random.rand() <= 0.7:\n",
    "            self.captured_args.append((*args, *tuple(kwargs.values())))\n",
    "        return self.unet(*args, **kwargs)\n",
    "\n",
    "def collect_calibration_data(ov_pipe, calibration_dataset_size: int, num_inference_steps: int) -> List[Dict]:\n",
    "    \n",
    "    original_unet = ov_pipe.transformer\n",
    "    calibration_data = []\n",
    "    disable_progress_bar(ov_pipe)\n",
    "    \n",
    "    dataset = datasets.load_dataset(\"google-research-datasets/conceptual_captions\", split=\"train\", trust_remote_code=True).shuffle(seed=42)\n",
    "\n",
    "    pipe_copy = ov_pipe\n",
    "    wrapped_unet = UNetWrapper(ov_pipe.transformer)\n",
    "    pipe_copy.transformer = wrapped_unet\n",
    "    # Run inference for data collection\n",
    "    pbar = tqdm(total=calibration_dataset_size)\n",
    "    for i, batch in enumerate(dataset):\n",
    "        prompt = batch[\"caption\"]\n",
    "        print(prompt)\n",
    "        if len(prompt) > ov_pipe.tokenizer.model_max_length:\n",
    "            continue\n",
    "        # Run the pipeline\n",
    "        ov_pipe(prompt, num_inference_steps=num_inference_steps)\n",
    "        calibration_data.extend(wrapped_unet.captured_args)\n",
    "        wrapped_unet.captured_args = []\n",
    "        pbar.update(len(calibration_data) - pbar.n)\n",
    "        if pbar.n >= calibration_dataset_size:\n",
    "            break\n",
    "\n",
    "    disable_progress_bar(ov_pipe, disable=False)\n",
    "    pipe_copy.transformer = original_unet\n",
    "    ov_pipe = pipe_copy\n",
    "    return calibration_data\n",
    "if to_quantize:\n",
    "    calibration_dataset_size = 300\n",
    "    unet_calibration_data = collect_calibration_data(pipe,\n",
    "                                                        calibration_dataset_size=calibration_dataset_size,\n",
    "                                                        num_inference_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "import nncf\n",
    "from nncf.quantization.advanced_parameters import AdvancedSmoothQuantParameters\n",
    "from nncf.quantization.range_estimator import RangeEstimatorParametersSet\n",
    "\n",
    "if to_quantize:\n",
    "    with disable_patching():\n",
    "        with torch.no_grad():\n",
    "            nncf.compress_weights(pipe.text_encoder)\n",
    "            nncf.compress_weights(pipe.text_encoder_2)\n",
    "            nncf.compress_weights(pipe.vae.encoder)\n",
    "            nncf.compress_weights(pipe.vae.decoder)\n",
    "            pipe.transformer = nncf.quantize(\n",
    "                model=pipe.transformer,\n",
    "                calibration_dataset=nncf.Dataset(unet_calibration_data),\n",
    "                subset_size=len(unet_calibration_data),\n",
    "                model_type=nncf.ModelType.TRANSFORMER,\n",
    "                ignored_scope=nncf.IgnoredScope(names=['conv2d']),\n",
    "                advanced_parameters=nncf.AdvancedQuantizationParameters(weights_range_estimator_params=RangeEstimatorParametersSet.MINMAX, activations_range_estimator_params=RangeEstimatorParametersSet.MINMAX)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "pipe.text_encoder = torch.compile(pipe.text_encoder, backend='openvino')\n",
    "pipe.text_encoder_2 = torch.compile(pipe.text_encoder_2, backend='openvino')\n",
    "pipe.vae.encoder = torch.compile(pipe.vae.encoder, backend='openvino')\n",
    "pipe.vae.decoder = torch.compile(pipe.vae.decoder, backend='openvino')\n",
    "pipe.transformer = torch.compile(pipe.transformer, backend='openvino')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap the Models\n",
    "\n",
    "Before inserting them, the models need to be wrapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_model(pipe_model, base_class, config=None):\n",
    "    class WrappedModel(base_class):\n",
    "        def __init__(self, model, config):\n",
    "            if(isinstance(config, dict)):\n",
    "                super().__init__(**config)\n",
    "            else:\n",
    "                super().__init__(config)\n",
    "            cls_name = base_class.__name__\n",
    "            if(cls_name=='AutoencoderKL'):\n",
    "                self.encoder = model.encoder\n",
    "                self.decoder = model.decoder\n",
    "            else:\n",
    "                self.model = model\n",
    "        def forward(self, *args, **kwargs):\n",
    "            return self.model(*args, **kwargs)\n",
    "    return WrappedModel(pipe_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "from diffusers.models.transformers.transformer_sd3 import SD3Transformer2DModel\n",
    "from diffusers.models.autoencoders.autoencoder_kl import AutoencoderKL\n",
    "from transformers.models.clip import CLIPTextModelWithProjection\n",
    "\n",
    "pipe.transformer = wrap_model(pipe.transformer, SD3Transformer2DModel, dict(transformer_config))\n",
    "pipe.vae = wrap_model(pipe.vae, AutoencoderKL, dict(vae_config))\n",
    "pipe.text_encoder = wrap_model(pipe.text_encoder, CLIPTextModelWithProjection, text_encoder_config)\n",
    "pipe.text_encoder_2 = wrap_model(pipe.text_encoder_2, CLIPTextModelWithProjection, text_encoder_2_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for Compilation\n",
    "\n",
    "Run inference with single step to compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "# Warmup the model for initial compile\n",
    "prompt = \"valley in the Alps at sunset, epic vista, beautiful landscape, 4k, 8k\"\n",
    "negative_prompt = \"frames, borderline, text, charachter, duplicate, error, out of frame, watermark, low quality, ugly, deformed, blur\"\n",
    "num_steps = 1\n",
    "with torch.no_grad():\n",
    "    image = pipe(prompt=prompt, negative_prompt=negative_prompt, num_inference_steps=num_steps, generator=generator).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "with torch.no_grad():\n",
    "    image = pipe(prompt=prompt, negative_prompt='', num_inference_steps=28, generator=generator, guidance_scale=5).images[0]\n",
    "image.resize((512, 512,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "def get_model_size(models):\n",
    "    total_size = 0\n",
    "    for model in models:\n",
    "        param_size = 0\n",
    "        for param in model.parameters():\n",
    "            param_size += param.nelement() * param.element_size()\n",
    "        buffer_size = 0\n",
    "        for buffer in model.buffers():\n",
    "            buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "        model_size_mb = (param_size + buffer_size) / 1024**2\n",
    "\n",
    "        total_size += model_size_mb\n",
    "    return total_size\n",
    "print(\"Transformer Size:\")\n",
    "print(get_model_size([pipe.transformer]))\n",
    "print(\"Pipeline Size:\")\n",
    "get_model_size([pipe.transformer, pipe.vae.encoder, pipe.vae.decoder, pipe.text_encoder, pipe.text_encoder_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "from sd3_quantization_helper import visualize_results\n",
    "\n",
    "opt_image = pipe(\n",
    "    \"A raccoon trapped inside a glass jar full of colorful candies, the background is steamy with vivid colors\",\n",
    "    negative_prompt=\"\",\n",
    "    num_inference_steps=28 if not use_flash_lora.value else 4,\n",
    "    guidance_scale=5 if not use_flash_lora.value else 0,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    generator=torch.Generator().manual_seed(141),\n",
    ").images[0]\n",
    "\n",
    "visualize_results(image, opt_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
